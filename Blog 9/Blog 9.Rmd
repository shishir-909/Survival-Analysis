---
title: "Prediction Interval and its Calibration"
runningheader: "Tufte Handout with R Markdown" # only for pdf output
subtitle: "An Application in Reliability using R Statistical Software" # only for html output
author: "Shishir Rao"
date: "`r Sys.Date()`"
output:
  tufte::tufte_html: default
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_book:
    citation_package: natbib
    latex_engine: xelatex
bibliography: skeleton.bib
link-citations: yes
---

```{r setup, include=FALSE}
library(tufte)
library(knitr)
library(kableExtra)
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(cache.extra = packageVersion("tufte"), echo = FALSE)
options(htmltools.dir.version = FALSE)
library(survival)
library(tidyverse)
library(readr)

setwd("G:/My Drive/rao@ualberta.ca 2022-12-08 10 58/shishir@tamu.edu/My Drive/Interesting papers/Survival Models/GitHub/Survival/Survival-Analysis/Blog 9")
```

# Introduction

A potential customer would like to know how long will an expensive piece of industrial equipment they intend to purchase operate before failing. A maintenance manager needs to make a decision on whether an operating unit should be shut down and brought in for maintenance or let it run for some more time. A manufacturing company would like to allocate budget for potential warranty claim returns and would like to know the number of product failures in the next 6 months. 

Questions of the above nature can be addressed by analyzing data from past failures and constructing prediction intervals. In this blog article, we discuss prediction intervals, how are they different from confidence intervals and why is it necessary to calibrate prediction intervals. The methods mentioned in this blog post are from a chapter on prediction intervals in a textbook^[Statistical Methods for Reliability Data, Second Edition (William Q. Meeker, Luis A. Escobar, Francis G. Pascual)] that I have been reading. I highly recommend this book to anyone interested in applying statistical methods in the field of reliability engineering.

# Prediction Intervals vs Confidence Intervals

A prediction interval is an interval within which a future observation is likely to fall, whereas a confidence interval is an interval within which a population parameter is likely to fall. Lets look at this difference with the help of an example.

Suppose there are 1000 houses that are on sale in a particular neighborhood. We are interested in finding out the average sale price of a house. If we had access to the 1000 sale prices, finding the average is a straightforward exercise. Now, suppose that only 50 have been sold so far and we want to use this data to estimate the average sale price of the 1000 houses in the neighborhood. 

In the above example, our sample size is 50 and the population size is 1000. The population parameter that we want to estimate is the mean (or average) of 1000 homes ^[Other quantities like the median or any other quantile are also population parameters]. This population parameter is an unknown quantity, but it is fixed!^[We are talking about the frequentist approach. In the Bayesian approach, parameters are also treated as random quantities.] We can use the 50 sale prices to construct a confidence interval for the mean of 1000 sale prices. 

On the other hand, suppose we want to know the price range within which the next house that sells is going to fall in. This range would be the prediction interval. Note that the sale price of the next house that is sold is a random quantity, unlike the unknown population parameter which is fixed. The prediction that we are interested in is an "unknown and random" quantity whereas the average sale price of a house in the neighborhood is an "unknown but fixed" quantity. Confidence intervals are constructed for "unknown but fixed" population parameters^[Like the mean in this case] and prediction intervals are constructed for the "unknown and random" quantity.^[Sale price of the next house sold in this case.] Te rest of this article discusses prediction intervals and how to calibrate them.

# Reliability Applications

Similar to the example of house sale prices, consider a scenario where a customer wants to purchase an expensive piece of industrial equipment and is interested in the prediction interval for that particular equipment. The confidence interval would give the customer an idea of the mean time to failure of all identical equipment manufactured in the past, present and future. Although this is good information to have, the customer wants an interval for one equipment, which would also account for the individual variability in addition to the uncertainty in estimating a parameter. Due to this additional variability, the prediction intervals are wider than confidence intervals. 

We can use data from historical failures to get a prediction interval for an equipment. The difference between this case and the previous example of house prices is that historical data from equipment failures may be incomplete. This means that in addition to failures, the data might consist of censored observations, where the equipment hasn't failed by the end of the observation period^[This is for right censored observations. We can also have left or interval censored observation and even truncated observations in reliability data.].

In the following article, a dataset consisting of past failures (and censored) observations will be analyzed to construct a prediction interval. It will then be calibrated to ensure adequate coverage probability^[Will be discussed later in the article]. All computations are performed using R and the code is available [**here**](https://github.com/shishir-909/Survival-Analysis/blob/110b5a7735fb2f6e82aac9102ccfda336d3ca350/Blog%209/Blog%209.Rmd).

# Case Study: Mechanical Switches

## Data

Table 1 shows failure times^[in Millions of Operations] of 40 randomly selected mechanical switches tested in a facility. 3 switches had not failed by the end of the test, leading to right censored observations. This dataset is from an article by Nair (1984)^[Nair, V. N. (1984). Confidence bands for survival functions with censored data: A compar-
ative study. *Technometrics 26*, 265–275.]. It is also publicly available on Datashare, Iowa State University's open data repository and can be accessed through [**this link.**](https://iastate.figshare.com/articles/dataset/Reliability_Data_Laboratory_Failure-Time_and_Single-Time_Strength_Data/14454753?backTo=%2Fcollections%2F_%2F5395665&file=28330332)

```{r Load Table, echo=FALSE, tab.cap="Table 1. Mechanical Switches Life Test Data"}

MechanicalSwitch <- readr::read_csv("Data\\MechanicalSwitch.csv", show_col_types = FALSE)

knitr::kable(MechanicalSwitch,
             align = rep('c', 5),
             table.envir = 'table*') %>%
  kableExtra::kable_styling("striped", full_width = T, position = "left") %>%
  kableExtra::scroll_box(height = "200px")

```

There are 2 modes of failure - Spring A and Spring B^[The 2 springs are identical in construction, but are subject to different stresses, thereby leading to different time-to-failure probability distributions.]. We are interested in constructing a prediction interval for the time to failure of a mechanical switch, irrespective of mode of failure. Note that a proper analysis of this dataset should treat the two modes of failures as competing risks and apply appropriate methods that are capable of handling competing risks. We are not going to do that in this article. The purpose of this blog article is to discuss prediction intervals and their calibration. A simple parametric model ignoring competing risks will be assumed in the interest of a succinct article focusing on one topic i.e prediction intervals. 

## Model

The log location scale distribution model that we consider for this dataset is shown in Eq. A below.

$$
\hspace{-2cm} 
\begin{align*}
F(t;\mu,\sigma) & = Pr(T\le t)\\ & = \Phi \left[\frac{log(t)-\mu}{\sigma}\right]
\tag{Eq. A}
\end{align*}
$$

The distribution of $\Phi$ determines the time-to-failure distribution. For example, if $t$ is assumed to have a Weibull distribution, $\Phi$ is the cumulative distribution function (CDF) of the standard smallest extreme value (SEV) distribution. Similarly, if $t$ is assumed to have a log-normal distribution, then $\Phi$ is the CDF of the standard normal distribution.

Weibull and log-normal distributions are widely used to describe time-to-failure distributions in reliability applications. Other distributions^[Eg: exponential, log-logistic, generalized gamma, normal etc] can also be considered, depending on the application and motivated by the failure process. Only log-normal and Weibull distributions are considered as candidate models in this article.

Figure 1 shows a Weibull probability plot and Figure 2 shows a log-normal probability plot of the mechanical switches dataset.

```{r Weibull plot, echo=FALSE, fig.cap="Figure 1: Weibull probability plot"}
MechanicalSwitch <- MechanicalSwitch %>%
  dplyr::mutate(delta = ifelse(MechanicalSwitch$`Failure Mode` == "Censored",0,1)) %>%
  dplyr::rename(Operations = `Millions of Operations`)

# Weibull plot

#Points

fit_3 <- survival::survfit(Surv(Operations, delta) ~ 1, data = MechanicalSwitch)

#summary(fit_3)

CDF_df <- data.frame(Operations = fit_3$time,
                     p = 1 - fit_3$surv) %>%
  dplyr::distinct(p, .keep_all = TRUE) #This line is required. Otherwise, the time column includes all observations. I just need the observations where there is a jump in the CDF.

CDF_df <- CDF_df %>%
      dplyr::mutate(bottomOfStairs = c(0,head(p,-1))) %>%
      dplyr::mutate(middleOfStairs = (p + bottomOfStairs)/2) %>% #Note: p is top of stairs.
      dplyr::select(!p) %>%
      dplyr::rename(p = middleOfStairs) #This piece of code is for plotting the CDF at the mid point of the jumps instead of plotting at the top of the jumps. See Section 6.4.2 of Chapter 6 in SMRD2.
     
#Line. 

fit.weibull <- survival::survreg(Surv(Operations, delta) ~ 1, data = MechanicalSwitch, dist = "weibull")

#summary(fit.weibull)

mu.ML_Weibull <- fit.weibull$coefficients
se_mu.ML_Weibull <- sqrt(fit.weibull[["var"]][1,1])
sigma.ML_Weibull <- fit.weibull$scale
se_sigma.ML_Weibull <- sqrt((sigma.ML_Weibull**2)*fit.weibull[["var"]][2,2])
AIC.weibull <- round(AIC(fit.weibull),2)


#Dashed lines. Prepare a dataframe to hold values of the CI for F(t). We will use the CI based on "e", as done in chunk 4

t <-seq(1.1,3.8,length = 10000) #Range of data used
e <- (log(t) - mu.ML_Weibull)/sigma.ML_Weibull

dgdMu <- (-1/sigma.ML_Weibull)*(exp(-exp(e)))*(exp(e))
dgdSigma <- (-e/sigma.ML_Weibull)*(exp(-exp(e)))*(exp(e))

se_Ft.ML_Weibull <- sqrt(((dgdMu**2)*(se_mu.ML_Weibull**2)) + (2*(dgdMu)*(dgdSigma)*(sigma.ML_Weibull*fit.weibull[["var"]][1,2])) + ((dgdSigma**2)*(se_sigma.ML_Weibull**2)))

#Confidence interval based on "e".

se_e_Weibull <- (1/sigma.ML_Weibull)*sqrt((se_mu.ML_Weibull**2) + (2*e*(sigma.ML_Weibull*fit.weibull[["var"]][1,2])) + ((e**2)*(se_sigma.ML_Weibull**2)))

CI_lower_e_Weibull <- e - (qnorm(0.975)*se_e_Weibull)
CI_upper_e_Weibull <- e + (qnorm(0.975)*se_e_Weibull)

CI_lower_eFt_Weibull <- 1 - exp(-exp(CI_lower_e_Weibull))
CI_upper_eFt_Weibull <- 1 - exp(-exp(CI_upper_e_Weibull))

CDF_CI_Weibull_df <- data.frame(Operations = t,
                                CI_Lower = CI_lower_eFt_Weibull,
                                CI_Upper = CI_upper_eFt_Weibull)

#Transformers

xTransformer.weibull <- scales::trans_new(
  name = "weibull.x.transform",
  transform = function(t.p){
    x = log(t.p)
    return(x)
  },
  inverse = function(x) {
    t.p = exp(x)
    return(t.p)
  }
)


yTransfomer.weibull <- scales::trans_new(
  name = "weibull.y.transform",
  transform = function(p){
    y = log(-log(1-p))
    return(y)
  },
  inverse = function(y) {
    p = 1 - exp(-exp(y))
    return(p)
  }
)

gg.weibull <- ggplot() + geom_point(data = CDF_df, aes(x = Operations, y = p)) + geom_line(data = CDF_CI_Weibull_df,
              aes(x = Operations, y = CI_Lower),
              linetype = "dashed") + geom_line(data= CDF_CI_Weibull_df, aes(x = Operations, y = CI_Upper), linetype = "dashed") + geom_abline(
  aes(intercept = (-fit.weibull$coefficients/fit.weibull$scale), #y-axis intercept
      slope = 1/fit.weibull$scale)
)  + scale_x_continuous(
  transform = xTransformer.weibull,
  name = "Millions of Operations",
  breaks = seq(1,4, by = 0.5)
) + scale_y_continuous(
  transform = yTransfomer.weibull,
  name = "Fraction Failing",
  breaks = seq(0.0,1,0.1)
) #+ ggtitle(label = "Weibull Probability Plot")

gg.weibull
```


```{r log-normal plot, echo=FALSE, fig.cap="Figure 2: log-normal probability plot"}
#log-normal plot

fit.logNormal <- survival::survreg(Surv(Operations, delta) ~ 1, data = MechanicalSwitch, dist = "lognormal")

#summary(fit.logNormal)

mu.ML <- fit.logNormal$coefficients
sigma.ML <- fit.logNormal$scale
AIC.lognormal <- round(AIC(fit.logNormal),2)

#Points (We already have this from the Weibull plot)

#Line. Note that lognormal ML fit already available from fit.logNormal

mu.ML_logNormal <- fit.logNormal$coefficients
se_mu.ML_logNormal <- sqrt(fit.logNormal[["var"]][1,1])
sigma.ML_logNormal <- fit.logNormal$scale
se_sigma.ML_logNormal <- sqrt((sigma.ML_logNormal**2)*fit.logNormal[["var"]][2,2])

#Dashed lines. Prepare a dataframe to hold values of the CI for F(t). We will use the CI based on "e", as done in chunk 5

t <-seq(1.1,3.8,length = 10000) #Range of data used
e <- (log(t) - mu.ML_logNormal)/sigma.ML_logNormal

Ft.ML_Lognormal <- pnorm(e)

#To get standard error of the above CDF, use equation 8.8 from SMRD2. My proof for 8.8 is here: https://drive.google.com/open?id=12D9tcASJAgxY2kecw2tbCGUK9a8RfrQK&usp=drive_fs

dgdMu <- (-1/sigma.ML_logNormal)*dnorm(e)
dgdSigma <- (-e/sigma.ML_logNormal)*dnorm(e)

se_Ft.ML_logNormal <- sqrt(((dgdMu**2)*(se_mu.ML_logNormal**2)) + (2*(dgdMu)*(dgdSigma)*(sigma.ML_logNormal*fit.logNormal[["var"]][1,2])) + ((dgdSigma**2)*(se_sigma.ML_logNormal**2)))

#Confidence interval based on "e".

se_e_logNormal <- (1/sigma.ML_logNormal)*sqrt((se_mu.ML_logNormal**2) + (2*e*(sigma.ML_logNormal*fit.logNormal[["var"]][1,2])) + ((e**2)*(se_sigma.ML_logNormal**2)))

CI_lower_e_logNormal <- e - (qnorm(0.975)*se_e_logNormal)
CI_upper_e_logNormal <- e + (qnorm(0.975)*se_e_logNormal)

CI_lower_eFt_logNormal <- pnorm(CI_lower_e_logNormal)
CI_upper_eFt_logNormal <- pnorm(CI_upper_e_logNormal)

CDF_CI_logNormal_df <- data.frame(Operations = t,
                                CI_Lower = CI_lower_eFt_logNormal,
                                CI_Upper = CI_upper_eFt_logNormal)

#Transformers

xTransformer.logNormal <- scales::trans_new(
  name = "logNormal.x.transform",
  transform = function(t.p){
    x = log(t.p)
    return(x)
  },
  inverse = function(x) {
    t.p = exp(x)
    return(t.p)
  }
)


yTransfomer.logNormal <- scales::trans_new(
  name = "logNormal.y.transform",
  transform = function(p){
    y = qnorm(p)
    return(y)
  },
  inverse = function(y) {
    p = pnorm(y)
    return(p)
  }
)

gg.logNormal <- ggplot() + geom_point(data = CDF_df, aes(x = Operations, y = p)) + geom_line(data = CDF_CI_logNormal_df,
              aes(x = Operations, y = CI_Lower),
              linetype = "dashed") + geom_line(data= CDF_CI_logNormal_df, aes(x = Operations, y = CI_Upper), linetype = "dashed") + geom_abline(
  aes(intercept = (-fit.logNormal$coefficients/fit.logNormal$scale), #y-axis intercept
      slope = 1/fit.logNormal$scale)
)  + scale_x_continuous(
  transform = xTransformer.logNormal,
  name = "Millions of Operations",
  breaks = seq(1,4, by = 0.5)
) + scale_y_continuous(
  transform = yTransfomer.logNormal,
  name = "Fraction Failing",
  breaks = seq(0.0,1,0.1)
) #+ ggtitle(label = "log-normal Probability Plot")

gg.logNormal

#gridExtra::grid.arrange(gg.weibull, gg.logNormal, ncol = 2)

```

The log-normal model looks like a better fit to the data as compared to the Weibull model, especially at the lower tail. The AIC of the Weibull model is `r AIC.weibull` and the AIC of the log-normal model is `r AIC.lognormal`, further reinforcing that the log-normal is a better choice for this data. The maximum likelihood (ML) estimates of the parameters of the log-normal distribution and their standard errors are shown in the table 2 below. 


```{r ML estimates table, echo=FALSE, tab.cap="Table 2. Maximum Likelihood estimates of the log-normal model"}
ML_estimates <- data.frame(c(mu.ML_logNormal,
                             se_mu.ML_logNormal,
                             sigma.ML_logNormal,
                             se_sigma.ML_logNormal))
rownames(ML_estimates) <- c("$\\hat{\\mu}$","$\\mathrm SE_{\\hat{\\mu}}$","$\\hat{\\sigma}$","$\\mathrm SE_{\\hat{\\sigma}}$")
colnames(ML_estimates) <- c("Estimate")

knitr::kable(ML_estimates, align = rep('c', 1), table.envir = 'table*', escape = FALSE, digits = 3) %>%
 kableExtra::kable_styling("striped", full_width = T, position = "left") 

```

## Plug-in Prediction Interval

```{r Plug-in interval}
T.tilda.lower <- round(exp(mu.ML + (qnorm(0.05)*sigma.ML)),2)
T.tilda.upper <- round(exp(mu.ML + (qnorm(0.95)*sigma.ML)),2)

#The 2 sided 90% prediction interval is (T.tilda.lower, T.tilda.upper)
```

Now that we have the ML estimates of the log-normal distribution parameters, we can easily obtain the 90% approximate^[As opposed to an "exact" prediction interval where the confidence level is exactly met.] prediction interval. A one sided lower 95% approximate prediction bound and a one sided upper 95% approximate prediction bound are combined to form an equal tailed two sided 90% approximate prediction interval. Although it might be possible to find a narrower interval with unequal upper and lower tail probabilities that add to $\alpha$ = 10%, two-sided intervals are often reported in reliability applications even when the primary interest is on one side or the other. With the appropriate adjustment to the confidence level, a two-sided interval can be correctly interpreted as a one-sided prediction bound. 
For the mechanical switches, a two sided approximate 90% prediction interval calculated using the ML estimates of the parameters is [`r T.tilda.lower`,`r T.tilda.upper`].

## Calibrating Prediction Intervals

The method of finding the approximate prediction interval described in the previous section is a "naive" method because it simply uses the ML estimates of the parameters. It ignores the uncertainty in the estimated parameter values, which then leads to the coverage probability^[Coverage probability is the probability that a prediction interval procedure gives an interval containing the future prediction.] being generally smaller than the nominal confidence level. When there is a lot of information in the data set i.e large number of failures and little censoring, the naive plug-in method may suffice. If this is not the case, it is necessary to refine/calibrate the interval to ensure better coverage probability. 

Escobar and Meeker (1999)^[Escobar, L. A. and W. Q. Meeker (1999). Statistical prediction based on censored life data. *Technometrics 41*, 113–124.], discuss different prediction interval calibration methods in their paper. Here, we use the method where calibration is done by simulating *B*^[*B* is typically chosen between 10,000 and 50,000.] realizations of the data and averaging the conditional coverage probabilities. A comprehensive discussion on this method is outside the scope of this blog article and I encourage you to read the paper or the textbook that I have mentioned previously. A summary of the steps involved in this method is shown below: 

*Step 1*: Choose a particular value of the one sided confidence level $1 - \alpha_{c}$. In our case study, the one sided confidence level is 5%. So, we can start with $\alpha_{c} = 0.05$.\
*Step 2*: Simulate a realization of the available data using fractional random weight bootstrap^[Note on fractional random weight bootstrap method is included in the next section.] sampling.\
*Step 3*: Find the maximum likelihood estimates of the distribution parameter for the data simulated in Step 2.\
*Step 4*: Find the one sided upper 95% prediction interval from the parameter estimates in step 3.\
*Step 5*: Find the coverage probability of the interval from Step 4 using the original ML estimates of the parameters in Table 2.\
*Step 6*: Repeat Steps 2,3,4 and 5 $B$ times. $B = 10,000$ has been used in this case.\
*Step 7*: Take the average of the 10,000 coverage probabilities from Step 5. If this value is less than our target confidence level of 95%, decrease the value of $\alpha_{c}$ in Step 1 and repeat the whole procedure until the average in Step 7 is at or above our target confidence level.\

The above procedure will give us a calibrated one sided approximate 95% upper prediction bound. Repeat the same procedure to find the calibrated one sided approximate 95% lower prediction bound by modifying Step 4 to calculate the lower bound instead of the upper bound.

Combining the two one-sided 95% bounds gives a two-sided 90% interval. Table 3 below shows the calibrated interval along with the naive plug in interval.

```{r Calibration, include=FALSE}
start.time <- Sys.time()
B <- 10000
n <- nrow(MechanicalSwitch)
r <- sum(MechanicalSwitch$delta)
mu.B <- c()
sigma.B <- c()

alpha.c <- 1 - 0.96 #start with alpha.c as "1 - 0.95". Then, play around with it to find a value that gives CP.PI.PB.upper as 0.95. Then repeat process again to find a value that gives CP.PI.FRW.lower as 0.95. The formula for CP.PI.FRW.upper and CP.PI.PB.lower is given below.

i <- 1

while(i <= B){
  
  if(i%%1000 == 0){print(i)}
  
  #The next 3 lines of code are for generating the fractional random weights for simulation.
  gamma_randomNumbers <- rgamma(n, shape = 1, rate = 1)
dirichlet_randomNumbers <- gamma_randomNumbers/sum(gamma_randomNumbers)
fractional_randomWeights <- n*dirichlet_randomNumbers

fit_B <- survival::survreg(Surv(Operations, delta) ~ 1, data = MechanicalSwitch, weights = fractional_randomWeights,dist = "lognormal")

  mu.B[i] <- fit_B$coefficients
  sigma.B[i] <- fit_B$scale
  
  i <- i + 1
}

  T.tilda.lower.B <- exp(mu.B + (qnorm(alpha.c)*sigma.B))
  T.tilda.upper.B <- exp(mu.B + (qnorm(1-alpha.c)*sigma.B))

#The next 2 lines of code gives the coverage probability for the upper and lower bounds. See section 3.3 in the paper linked here: https://drive.google.com/open?id=1yqxRGOqa1-FVEou9B8jw89xQSjmYPiij&usp=drive_fs   
  
(CP.PI.FRW.upper <- ((1/B)*sum(pnorm((log(T.tilda.upper.B)-mu.ML)/sigma.ML))))

(CP.PI.FRW.lower <- ((1/B)*sum(1-pnorm((log(T.tilda.lower.B)-mu.ML)/sigma.ML))))

  #The calibrated values of the prediction bounds/interval shown below. This depends on the value of alpha.c that gives approx 95% coverage probability. This will be different for the upper and lower bounds. You will have to play around with it. See page 2 in the link here: https://drive.google.com/open?id=1Lf45d_og0teSJCYXarHQp5rxaCOofnSc&usp=drive_fs

T.tilda.lower.calib.FRW <- round(exp(mu.ML + (qnorm(1-0.96)*sigma.ML)),2)
T.tilda.upper.calib.FRW <- round(exp(mu.ML + (qnorm(0.96)*sigma.ML)),2)


end.time <- Sys.time()

end.time - start.time
```


```{r Prediction final table, tab.cap="Table 3: Approximate 90% Prediction Intervals"}
pred.int.df <- data.frame(naive = paste0("[",T.tilda.lower,", ",T.tilda.upper,"]"),
                          calibrated = paste0("[",T.tilda.lower.calib.FRW,", ",T.tilda.upper.calib.FRW,"]"))

colnames(pred.int.df) <- c("Naive 90% Prediction Interval", "Calibrated 90% Prediction Interval")

knitr::kable(pred.int.df, align = rep('c', 2), table.envir = 'table*', escape = FALSE, digits = 3) %>%
 kableExtra::kable_styling("striped", full_width = T, position = "left") 
```

Notice that the width of the calibrated interval is greater than the width of the naive interval. The calibrated approximate prediction interval is expected to have better coverage properties than the naive approximate prediction interval.

## Note on Fractional Random Weight Bootstrap

With a traditional bootstrap approach, we randomly sample data (from Table 1), with replacement. In a given bootstrap sample, some rows from table 1 may appear more than once while other may not appear at all. Think of the number of times that a particular row appears in a bootstrap sample as the "weight" for that observation. These weights are integer values and can take on values between 0 and $n$, where $n$ is the number of data points in a bootstrap sample. Note that these weights are positive "integer" values. When there is heavy censoring in the data, a bootstrap sample might consist of only censored observations, or very few failures. This leads to estimation problems when we try to estimate parameters using maximum likelihood. An alternative to these "integer weights" is "fractional weights", where each row in Table 1 gets assigned random non-integer weights. These non-integer weights can be generated from a continuous distribution of a random variable that has the same mean and standard deviation as the integer resampling weights. Xu et al. (2020)^[Xu, L., C. Gotwalt, Y. Hong, C. B. King, and W. Q. Meeker (2020). Applications of the fractional‐random‐weight bootstrap. *The American Statistician 74*, 345–358.] discusses an application of this method and shows how to generate the random weights. Although we don't have heavy censoring in our case and would probably not have encountered any estimation problems, we have still used fractional random weight bootstrapping to be on the safer side.

# Conclusion

In this blog post, we discussed prediction intervals and how to construct them using an example of a data set that contained time-to-failure of 40 mechanical switches. We also touched on the calibration of the interval to account for uncertainty in the estimates of the distribution parameters. Based on the analysis of the mechanical switches dataset, we can say with 90% confidence that the time-to-failure prediction for a future mechanical switch can be expected to fall in the range of [`r T.tilda.lower.calib.FRW`,`r T.tilda.upper.calib.FRW`] millions of operations.

This was a simple application of prediction intervals where all the equipment started testing at time zero and only three were censored observations. The concept here can be used to find prediction intervals for individual equipment that have been operating for different durations and operating under different conditions that would lead to different failure time distributions, using regression analysis. 

# Acknowledgements

1. Most of what I have learnt about time-to-event analysis is from the book Survival Analysis: Techniques for Censored and Truncated Data, Second Edition (John P. Klein and Melvin L. Moescheberger) and Statistical Methods for Reliability Data, Second Edition (William Q. Meeker, Luis A. Escobar, Francis G. Pascual).

2. All analyses were performed using R Statistical Software(v4.4.1; R Core Team 2024). The *survival* R package(v3.7.0; Therneau T 2024) was extensively used. Please refer to next section for all the packages used, their versions and the names of the package developers who deserve credit for building these amazing open source packages.

3. This article's format is a style that Edward Tufte uses in his books and handouts. I have used the *tufte* library in R and am grateful to the authors of this package. 

# Credits for R packages used

```{r Credits for packages, echo=FALSE}
report::cite_packages()
```

# End

I hope you found this article informative! If you have any comments or suggestions or if you find any errors and are keen to help correct the error, please write to me at shishir909@gmail.com.


```{r DO NOT USE, eval=FALSE, include=FALSE}

start.time <- Sys.time()
B <- 10000
n <- nrow(MechanicalSwitch)
r <- sum(MechanicalSwitch$delta)
mu.B <- c()
sigma.B <- c()

alpha.c <- 1 - seq(0.93,0.995, by = 0.001)

#alpha.c <- 1 - 0.96 #start with alpha.c as "1 - 0.95". Then, play around with it to find a value that gives CP.PI.PB.upper as 0.95. Then repeat process again to find a value that gives CP.PI.FRW.lower as 0.95. The formula for CP.PI.FRW.upper and CP.PI.PB.lower is given below.

j <- 1
CP.PI.FRW.upper <- c()
CP.PI.FRW.lower <- c()

while(j <= length(alpha.c)){

i <- 1

while(i <= B){
  
  if(i%%1000 == 0){#print(i)
    cat("\nj = ",j," & i =",i)}
  
  #The next 3 lines of code are for generating the fractional random weights for simulation.
  gamma_randomNumbers <- rgamma(n, shape = 1, rate = 1)
dirichlet_randomNumbers <- gamma_randomNumbers/sum(gamma_randomNumbers)
fractional_randomWeights <- n*dirichlet_randomNumbers

fit_B <- survival::survreg(Surv(Operations, delta) ~ 1, data = MechanicalSwitch, weights = fractional_randomWeights,dist = "lognormal")

  mu.B[i] <- fit_B$coefficients
  sigma.B[i] <- fit_B$scale
  
  i <- i + 1
}

  T.tilda.lower.B <- exp(mu.B + (qnorm(alpha.c[j])*sigma.B))
  T.tilda.upper.B <- exp(mu.B + (qnorm(1-alpha.c[j])*sigma.B))

#The next 2 lines of code gives the coverage probability for the upper and lower bounds. See section 3.3 in the paper linked here: https://drive.google.com/open?id=1yqxRGOqa1-FVEou9B8jw89xQSjmYPiij&usp=drive_fs   
  
(CP.PI.FRW.upper[j] <- ((1/B)*sum(pnorm((log(T.tilda.upper.B)-mu.ML)/sigma.ML))))

(CP.PI.FRW.lower[j] <- ((1/B)*sum(1-pnorm((log(T.tilda.lower.B)-mu.ML)/sigma.ML))))
  
  j <- j + 1
}



CP.PI.FRW.upper

calibrated.probs.df <- data.frame(oneMinusAlphac = 1 - alpha.c,
                                  CP.PI.FRW.lower = CP.PI.FRW.lower,
                                  CP.PI.FRW.upper = CP.PI.FRW.upper)

ggplot() + geom_line(data = calibrated.probs.df,
                     aes(x = oneMinusAlphac, y = CP.PI.FRW.lower), col = "blue") + geom_line(data = calibrated.probs.df,
                     aes(x = oneMinusAlphac, y = CP.PI.FRW.upper), col = "red")

end.time <- Sys.time()

end.time - start.time



```


```{r bib, include=FALSE}
# create a bib file for the R packages used in this document
knitr::write_bib(c('base', 'rmarkdown'), file = 'skeleton.bib')
```
