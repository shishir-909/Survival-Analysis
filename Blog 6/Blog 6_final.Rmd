---
title: "Parametric Modeling of Failure Time data: Part II"
runningheader: "Tufte Handout with R Markdown" # only for pdf output
subtitle: "Application using R Statistical Software" # only for html output
author: "Shishir Rao"
date: "`r Sys.Date()`"
output:
  tufte::tufte_html: default
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_book:
    citation_package: natbib
    latex_engine: xelatex
bibliography: skeleton.bib
link-citations: yes
---


```{r Load Libraries, include=FALSE}
library(tufte)
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(cache.extra = packageVersion('tufte'))
options(htmltools.dir.version = FALSE)
library(survival)
library(tidyverse)
library(readr)
library(survminer)
library(kableExtra)
library(flexsurv)
library(gridExtra)
library(km.ci)

setwd("G:/My Drive/rao@ualberta.ca 2022-12-08 10 58/shishir@tamu.edu/My Drive/Interesting papers/Survival Models/GitHub/Survival/Survival-Analysis/Blog 6")
```

# Introduction

This is the second part of the article on parametric modeling^[You can find the first part [**here**](https://rpubs.com/shishir909/1251052)]. In the previous part, we fit a Weibull regression model to a time-to-failure dataset from a life test on ceramic bearings. In this article, we will see if we can improve on the previous model by fitting a different model that can account for a varying shape parameter across different levels of the independent variable.

Similar to the first part, I have not included any R code within the article, but they are available on my Github page and can be accessed [**here**](https://github.com/shishir-909/Survival-Analysis/blob/931301ffee99b54144d53f63936ae3a09abe5aa1/Blog%206/Blog%206_final.Rmd).

# Weibull Regression

```{r Load Table, include=FALSE, tab.cap="Table"}
ceramic <- read_csv("Data/CeramicBearing02.csv", show_col_types = FALSE)

knitr::kable(ceramic, align = rep('c', 5), table.envir = 'table*') %>%
 kableExtra::kable_styling("striped", full_width = T, position = "left") %>%
kableExtra::scroll_box(height = "200px")

#, caption = "Table 1. Ceramic Bearings Life Test Data"
```


```{r Rename Columns, include=FALSE}
ceramic <- ceramic %>%
  dplyr::rename(Revolutions = `Millions of Revolutions`,
                Stress = `Stress (Mpsi)`) %>%
  dplyr::mutate(delta = rep(1,nrow(ceramic))) #No censored observations

# ceramic <- ceramic %>%
#   dplyr::filter(Revolutions != 0.012)

```

The log location scale distribution model used in the first part of the article is shown in Eq. A below.

$$
\hspace{-4cm} 
\begin{align*}
F(t;\beta_{0},\beta_{1},\sigma) & = Pr(T\le t)\\ 
& = \Phi\left[\frac{log(t)-(\beta_{0}+\beta_{1}log(stress))}{\sigma}\right]
\tag{Eq. A}
\end{align*}
$$
In this model, $\sigma$ remains constant across all levels of the explanatory variable, stress. $\Phi$ is the CDF of the standard smallest extreme value distribution. The paper by McCool(1980)^[McCool, J. I. (1980). Confidence limits for Weibull regression with censored data. *IEEE Transactions on Reliability 29*, 145â€“150.] mentions that $\sigma$ varying with stress has been frequently observed in rolling contact fatigue testing. The plot of standardized residuals vs fitted values^[Figure 7 in the [**previous article**](https://rpubs.com/shishir909/1251052).] also shows that the spread of the residuals is not the same across all four levels of stress. This suggests that a model with varying $\sigma$^[Shape parameter = $1/\sigma$.], as shown in Eq. B below, is worth considering.

$$
\hspace{-4cm} 
\begin{align*}
F(t;\beta_{0},\beta_{1},\beta^{*}_{0},\beta^{*}_{1}) & = Pr(T\le t)\\ 
& = \Phi\left[\frac{log(t)-(\beta_{0}+\beta_{1}log(stress))}{exp(\beta^{*}_{0} + \beta^{*}_{1}log(stress)}\right]
\tag{Eq. B}
\end{align*}
$$
where,

$$
log(\sigma_{i}) = \beta^{*}_{0} + \beta^{*}_{1}log(stress)
$$
Table 1 shows the ML estimates, standard errors and the Wald 95% confidence intervals of $\beta_{0}$, $\beta_{1}$, $\beta^{*}_{0}$ and $\beta^{*}_{1}$ from the model in Eq.B.

```{r IRLS computations continuous varSig, include=FALSE}
#My manual notes here: https://drive.google.com/open?id=1-MXUBr5OvIk7O1HKI27Cd-jBnlNdB0E6&usp=drive_fs

ceramic <- ceramic %>%
  dplyr::mutate(logStress = log(Stress)) %>%
  dplyr::mutate(y = log(Revolutions))

iterest <- function(beta.0, beta.1, beta.0s, beta.1s){
  
  beta.vector <- matrix(c(beta.0, beta.1), nrow = 2)
  X.matrix <- matrix(c(rep(1,dim(ceramic)[1]),ceramic$logStress), nrow = dim(ceramic)[1])
  n <- nrow(X.matrix)
  n.F <- sum(ceramic$delta)
  sigma.i <- exp(beta.0s + (beta.1s*ceramic$logStress))
  
  r.i <- (ceramic$y - (X.matrix %*% beta.vector))/sigma.i
  
  ceramic.1 <- ceramic %>% 
    dplyr::mutate(r.i = r.i,
                  sigma.i = sigma.i) %>%
    dplyr::mutate(u = case_when(delta == 1 ~ ((-1/sigma.i)*(1-exp(r.i))),
                                delta == 0 ~ ((1/sigma.i)*exp(r.i)))
                  ) %>%
    dplyr::mutate(diag.A = case_when(delta == 1 ~ ((-1/(sigma.i**2))*exp(r.i)),
                                     delta == 0 ~ ((-1/(sigma.i**2))*exp(r.i))))
  
  #w.r.i <- (exp(r.i)-1)/r.i
  
  u = ceramic.1$u
  D = X.matrix
  A = diag((-1)*c(ceramic.1$diag.A))
  
  weighted.regr.y <- (solve(A) %*% u) + (D %*% beta.vector)
  
  lm.fit <- lm(weighted.regr.y ~ D[,2], weights = diag(A))
  
  beta.0.star <- lm.fit$coefficients[1]
  beta.1.star <- lm.fit$coefficients[2]
  
  beta.vector.star <- matrix(c(beta.0.star, beta.1.star), nrow = 2)
  
  # sigma.star.squared <- (1/n)*sum(t(w.r.i) %*% (IRLSGreenTable2Data$y - (X.matrix %*% beta.vector.star))**2)
  # 
  # sigma.star <- sqrt(sigma.star.squared)
  
  ceramic.F <- ceramic.1 %>%
    dplyr::filter(delta==1) %>%
    dplyr::rowwise() %>%
    dplyr::mutate(Fl = ((y - (c(1,logStress) %*% beta.vector.star))**2)*((exp(r.i)-1)/r.i)/(exp(2*beta.1s*logStress)))
  
  # superAlloy.C <- superAlloy.1 %>%
  #   dplyr::filter(delta==0) %>%
  #   dplyr::rowwise() %>%
  #   dplyr::mutate(Ce = ((y - (c(1,logStress,logStress2) %*% beta.vector.star))**2)*((exp(r.i))/r.i)/(exp(2*beta.1s*logStress)))
  
  exp2beto.0s <- (1/(n.F))*(sum(ceramic.F$Fl)) 
                            # + sum(superAlloy.C$Ce))
                            # 
  beta.0s.star <- (1/2)*log(exp2beto.0s)
  
  ceramic.F1 <- ceramic.1 %>%
    dplyr::filter(delta==1) %>%
    dplyr::rowwise() %>%
    dplyr::mutate(Fl = ((y - (c(1,logStress) %*% beta.vector.star))**2)*((exp(r.i)-1)/r.i)*(logStress)/(exp(2*beta.0s.star)))
  
  # superAlloy.C1 <- superAlloy.1 %>%
  #   dplyr::filter(delta==0) %>%
  #   dplyr::rowwise() %>%
  #   dplyr::mutate(Ce = ((y - (c(1,logStress,logStress2) %*% beta.vector.star))**2)*((exp(r.i))/r.i)*(logStress)/(exp(2*beta.0s.star)))
  
  beta.1sfn <- function(beta.1s){
    sum(ceramic.F1$Fl/exp(2*beta.1s*ceramic.F1$logStress)) - sum(ceramic.F1$logStress)
    # + sum(superAlloy.C1$Ce/exp(2*beta.1s*superAlloy.C1$logStress)) 
  }
  
  #uniroot(beta.1sfn, c(-1000,1000))
  beta.1s.star <- uniroot(beta.1sfn, c(-10,10))$root
  
  return(c(beta.0.star,beta.1.star,beta.0s.star,beta.1s.star))
  
}

# beta.0 <- 1
# beta.1 <- 0
# #beta.2 <- 0
# beta.0s <- 1
# beta.1s <- 0
# 
# beta.0 <- beta.0.star
# beta.1 <- beta.1.star
# #beta.2 <- beta.2.star
# beta.0s <- beta.0s.star
# beta.1s <- beta.1s.star



beta.0_0 <- 1
beta.1_0 <- 0
#beta.2_0 <- 0
beta.0s_0 <- 1
beta.1s_0 <- 0
counter <- 1

    repeat {
            est.vector <- iterest(beta.0_0, beta.1_0,beta.0s_0,beta.1s_0)
            
            beta.0 <- est.vector[1]
            beta.1 <- est.vector[2]
            #beta.2 <- est.vector[3]
            beta.0s <- est.vector[3]
            beta.1s <- est.vector[4]
            
          if (abs(beta.0_0 - beta.0) & abs(beta.1_0 - beta.1) & abs(beta.0s - beta.0s_0) < 0.00001 & abs(beta.1s - beta.1s_0) < 0.00001) break
            # sometimes, I might have to play around with the required accuracy (0.0001 vs 0.000001) to get an approximate value
            beta.0_0 <- beta.0
            beta.1_0 <- beta.1
            #beta.2_0 <- beta.2
            beta.0s_0 <- beta.0s
            beta.1s_0 <- beta.1s
            counter <- counter + 1
            
            }

beta.0
beta.1
#beta.2
beta.0s
beta.1s

#Hessian 

beta.vector.star <- matrix(c(beta.0, beta.1), nrow = 2)
betaStar.vector.star <- matrix(c(beta.0s, beta.1s), nrow = 2)  

  ceramic.F <- ceramic %>%
    dplyr::filter(delta==1) %>%
    dplyr::rowwise() %>%
    dplyr::mutate(sigma.i = exp(c(1,logStress) %*% betaStar.vector.star)) %>%
    dplyr::mutate(r.i = (y - (c(1,logStress) %*% beta.vector.star))/sigma.i)  
  
  # superAlloy.C <- superAlloy %>%
  #   dplyr::filter(delta==0) %>%
  #   dplyr::rowwise() %>%
  #   dplyr::mutate(sigma.i = exp(c(1,logStress) %*% betaStar.vector.star)) %>%
  #   dplyr::mutate(r.i = (y - (c(1,logStress,logStress2) %*% beta.vector.star))/sigma.i) 
  
d2Ldbeta.02 <- H11 <- sum((-1/(ceramic.F$sigma.i**2))*exp(ceramic.F$r.i)) 

d2Ldbeta.0beta.1 <- H12 <- sum((-ceramic.F$logStress/(ceramic.F$sigma.i**2))*exp(ceramic.F$r.i)) 

d2Ldbeta.0beta.0s <- H13 <- sum((1/(ceramic.F$sigma.i))*(1 - exp(ceramic.F$r.i) - (ceramic.F$r.i*exp(ceramic.F$r.i))))

d2Ldbeta.0beta.1s <- H14 <- sum((ceramic.F$logStress/(ceramic.F$sigma.i))*(1 - exp(ceramic.F$r.i) - (ceramic.F$r.i*exp(ceramic.F$r.i))))

d2Ldbeta.12 <- H22 <- sum(-1/(ceramic.F$sigma.i**2)*(ceramic.F$logStress**2)*exp(ceramic.F$r.i)) 

d2Ldbeta.1beta.0s <- H23 <- sum((ceramic.F$logStress/ceramic.F$sigma.i)*(1 - exp(ceramic.F$r.i) - (ceramic.F$r.i*exp(ceramic.F$r.i))))

d2Ldbeta.1beta.1s <- H24 <- sum(((ceramic.F$logStress**2)/ceramic.F$sigma.i)*(1 - exp(ceramic.F$r.i) - (ceramic.F$r.i*exp(ceramic.F$r.i))))

d2Ldbeta.0s2 <- H33 <- sum((ceramic.F$r.i)*(1 - exp(ceramic.F$r.i) - (ceramic.F$r.i*exp(ceramic.F$r.i))))

d2Ldbeta.0sbeta.1s <- H34 <- sum((ceramic.F$logStress*ceramic.F$r.i)*(1 - exp(ceramic.F$r.i) - (ceramic.F$r.i*exp(ceramic.F$r.i))))

d2Ldbeta.1s2 <- H44 <- sum(((ceramic.F$logStress**2)*ceramic.F$r.i)*(1 - exp(ceramic.F$r.i) - (ceramic.F$r.i*exp(ceramic.F$r.i))))

hessian.matrix <- matrix(c(H11,H12,H13,H14,H12,H22,H23,H24,H13,H23,H33,H34,H14,H24,H34,H44), nrow = 4)

inf.matrix <- (-1)*hessian.matrix #Negative of Hessian is the information matrix when we maximize likelihood.

varcov <- solve(inf.matrix)

(std.er <- sqrt(diag(varcov)))

sigma.i_final <- exp(beta.0s + (beta.1s*ceramic$logStress))
r.i_final <- (ceramic$y-(beta.0 + (beta.1*ceramic$logStress)))/sigma.i_final

```

```{r Prep ML table 1, include=FALSE}
RegrParas <- data.frame(
  ML_est = c(beta.0, beta.1, beta.0s, beta.1s),
  Std.Err = std.er)

RegrParas <- RegrParas %>%
  dplyr::mutate(Wald_0.95_Lower = ML_est - (qnorm(0.975)*Std.Err),
                Wald_0.95_Upper = ML_est + (qnorm(0.975)*Std.Err))

rownames(RegrParas) <- c("$\\hat{\\beta_{0}}$", "$\\hat{\\beta_{1}}$", "$\\hat{\\beta^{*}_{0}}$", "$\\hat{\\beta^{*}_{1}}$")
colnames(RegrParas) <- c("ML estimate","Standard Error","Wald Lower 95% CI","Wald Upper 95% CI")

Regr_LL <- round(sum((-log(sigma.i_final)) + r.i_final - exp(r.i_final) - log(ceramic$Revolutions)),3)

chisqStat <- 2*(Regr_LL - (-54.402))

p.val <- 1 - pchisq(chisqStat,1)
```

```{r ML Table 1, echo=FALSE, tab.cap="Table 1. Maximum Likelihood estimates"}
knitr::kable(RegrParas, align = rep('c', 5), table.envir = 'table*', escape = FALSE, digits = 3) %>%
 kableExtra::kable_styling("striped", full_width = T, position = "left") 

```

The *survival* package in R does not support fitting parametric models where the shape parameter depends on the covariates, like Eq. B above. I coded the Iteratively Reweighted Least Squares^[P.J. Green. Iteratively reweighted least squares for maximum likelihood estimation, and some robust and resistant alternatives (with discussion). *J. Royal Stat. Soc. B*, 46:149 192, 1984.] algorithm to estimate the ML estimates and their standard errors, without using any packages^[I hope to write a separate blog article about this some other time.]. Alternatively (with some care), one could also write the log-likelihood equation and maximise it using optim() or maxLik() packages or use a package like *flexsurv* to directly fit parametric survival models that allow for a varying shape parameter. My limited experience in using these packages have resulted in some cases where the algorithm converged successfully while it didn't in other cases. The standard errors reported by these packages also depend on the scale of the ML estimates and can be misleading if the estimates are very different in scale^[See my note on CrossValidated [**here**](https://stats.stackexchange.com/questions/657281/note-on-standard-errors-reported-by-flexsurv-package-as-compared-to-survival)].

The log likelihood of the regression model is `r Regr_LL` with 4 parameters. A likelihood ratio (LR) test comparing regression model above with the regression model from the previous part is shown below.

$$
\begin{align*}
\hspace{-2cm} 
\chi^{2}_{LR} &  =  2(LL_{VarShapeRegr} - LL_{ConstShapeRegr})\\ & = 2(-53.08 - (-54.402)) = 2.644
\end{align*}
$$
The p-value for the LR test with the $\chi^{2}_{LR}$ statistic as shown above and 1 degree of freedom (4 parameters - 3 parameters) is 0.104. There is insufficient evidence that the varying shape parameter model is any better than the constant shape parameter model, at the 5% significance level. 

Fig.1 below shows the Weibull probability plot of the varying shape parameter model. Notice that the ML lines here are not parallel, like they were in the constant shape parameter model^[Fig. 6 in the [**previous article.**](https://rpubs.com/shishir909/1251052)]. Still, it doesn't look like the fit of the lines to the non-parametric CDF estimates are very different to the fit from the constant shape parameter model.


```{r Weibull plot varSig regression parameters, include=FALSE}
mu.0.87.varSig <- beta.0 + (beta.1*log(0.87))

sigma.0.87.varSig <- exp(beta.0s + (beta.1s*log(0.87)))

mu.0.99.varSig <- beta.0 + (beta.1*log(0.99))

sigma.0.99.varSig <- exp(beta.0s + (beta.1s*log(0.99)))

mu.1.09.varSig <- beta.0 + (beta.1*log(1.09))

sigma.1.09.varSig <- exp(beta.0s + (beta.1s*log(1.09)))

mu.1.18.varSig <- beta.0 + (beta.1*log(1.18))

sigma.1.18.varSig <- exp(beta.0s + (beta.1s*log(1.18)))


```


```{r Weibull plot prep, include=FALSE}
#Fig 17.7 a and b equivalent for ceramic data

##I need the Kaplan Meir estimates of the CDF for each level of stress. Instead of coding 4 separate times for 4 stress levels, I am going to loop it.

uniqueStress <- sort(unique(ceramic$Stress))
Fig17_CDF <- list() #Non parametric CDF estimates for all plots in Fig 17
Fig17.a_MLest <- list() #Maximum likelihood estimates of plot (a) i.e lognormal
Fig17.b_MLest <- list() #Maximum likelihood estimates of plot (b) i.e weibull
i <- 1
while(i <= length(uniqueStress)){
  df <- ceramic %>%
    dplyr::filter(Stress == uniqueStress[i])
  
  fit_df <- survival::survfit(Surv(Revolutions, delta) ~ 1, data = df)
  
  temp_df <- data.frame(Revolutions = fit_df$time,
                        p = 1-fit_df$surv)
  
  temp_df <- temp_df %>%
      dplyr::mutate(bottomOfStairs = c(0,head(p,-1))) %>%
      dplyr::mutate(middleOfStairs = (p + bottomOfStairs)/2) %>% #Note: p is top of stairs.
      dplyr::select(!p) %>%
      dplyr::rename(p = middleOfStairs) #This piece of code is for plotting the CDF at the mid point of the jumps instead of plotting at the top of the jumps. See Section 6.4.2 of Chapter 6 in SMRD2.
  
  Fig17_CDF[[i]] <- temp_df
  
  para_fit_logNormal <- survival::survreg(Surv(Revolutions, delta) ~ 1, data = df, dist = "lognormal")
  
  Fig17.a_MLest[[i]] <- c(
    para_fit_logNormal[["coefficients"]], #mu
    para_fit_logNormal[["scale"]], #sigma
    para_fit_logNormal[["loglik"]][2], #log likelihood
    sqrt(para_fit_logNormal[["var"]][1, 1]), #s.e of mu
    (para_fit_logNormal[["scale"]] * sqrt(para_fit_logNormal[["var"]][2, 2])) #s.e of scale
  ) 
  
  para_fit_weibull <- survival::survreg(Surv(Revolutions, delta) ~ 1, data = df, dist = "weibull")
  
  Fig17.b_MLest[[i]] <- c(
    para_fit_weibull[["coefficients"]], #mu
    para_fit_weibull[["scale"]], #sigma
    para_fit_weibull[["loglik"]][2], #log-likelihood
    sqrt(para_fit_weibull[["var"]][1, 1]), #s.e of mu
    (para_fit_weibull[["scale"]] * sqrt(para_fit_weibull[["var"]][2, 2])) #s.e of sigma
  )
  
  i = i + 1
   
}

names(Fig17_CDF) <- uniqueStress
names(Fig17.a_MLest) <- uniqueStress
names(Fig17.b_MLest) <- uniqueStress

#In order to linearize a log-normal distribution, log(t.p) vs phi inverse(p) plots as a straight line. See Section 6.2.3 in SMRD2. We will now code the 2 transformers for X and Y axes for log-normal

xTransformer.logNormal <- scales::trans_new(
  name = "logNormal.x.transform",
  transform = function(t.p){
    x = log(t.p)
    return(x)
  },
  inverse = function(x) {
    t.p = exp(x)
    return(t.p)
  }
)


yTransfomer.logNormal <- scales::trans_new(
  name = "logNormal.y.transform",
  transform = function(p){
    y = qnorm(p)
    return(y)
  },
  inverse = function(y) {
    p = pnorm(y)
    return(p)
  }
)

#In order to linearize a weibull distribution, log(t.p) vs log[-log(1-p)] plots as a straight line. See Section 6.2.4 in SMRD2. We will now code the 2 transformers for X and Y axes for weibull

xTransformer.weibull <- scales::trans_new(
  name = "weibull.x.transform",
  transform = function(t.p){
    x = log(t.p)
    return(x)
  },
  inverse = function(x) {
    t.p = exp(x)
    return(t.p)
  }
)


yTransfomer.weibull <- scales::trans_new(
  name = "weibull.y.transform",
  transform = function(p){
    y = log(-log(1-p))
    return(y)
  },
  inverse = function(y) {
    p = 1 - exp(-exp(y))
    return(p)
  }
)
```


```{r Weibull plot varSig Regression, echo=FALSE, fig.cap="Figure 1: Weibull plot of the varying shape regression model", cache=TRUE}
#Weibull plot 17.10 d

cols <- c("0.87 Mpsi"="red","0.99 Mpsi"="blue","1.09 Mpsi"="green","1.18 Mpsi"="orange")

ggplot() + geom_point(
  data = Fig17_CDF[["0.87"]],
  aes(x = Revolutions, y = p, color = "0.87 Mpsi")
  ) + geom_point(
  data = Fig17_CDF[["0.99"]],
  aes(x = Revolutions, y = p, color = "0.99 Mpsi"),
  ) + geom_point(
  data = Fig17_CDF[["1.09"]],
  aes(x = Revolutions, y = p, color = "1.09 Mpsi")
) + geom_point(
  data = Fig17_CDF[["1.18"]],
  aes(x = Revolutions, y = p, color = "1.18 Mpsi")
) + scale_x_continuous(
  transform = xTransformer.weibull,
  name = "Hours",
  breaks = c(10,50,100,250,500, 1000,1100)
) + scale_y_continuous(
  transform = yTransfomer.weibull,
  name = "Fraction Failing",
  breaks = seq(0.00,0.99,0.05)
) + geom_abline(
  aes(intercept = (-mu.0.87.varSig/sigma.0.87.varSig), #y-axis intercept
      slope = 1/sigma.0.87.varSig, color = "0.87 Mpsi")
) + geom_abline(
  aes(intercept = (-mu.0.99.varSig/sigma.0.99.varSig), #y-axis intercept
      slope = 1/sigma.0.99.varSig, color = "0.99 Mpsi")
) + geom_abline(
  aes(intercept = (-mu.1.09.varSig/sigma.1.09.varSig), #y-axis intercept
      slope = 1/sigma.1.09.varSig, color = "1.09 Mpsi")
) + geom_abline(
  aes(intercept = (-mu.1.18.varSig/sigma.1.18.varSig), #y-axis intercept
      slope = 1/sigma.1.18.varSig, color = "1.18 Mpsi")
) + ggtitle(label = "Weibull Plot for Varying Shape Regression Model") + scale_color_manual(name="Stress",values = cols)

```

Our attempt at trying to fit a slightly more complicated model did not result in a fit that is any significantly better than the constant shape model. 

McCool(1981) mentions in his paper that he **conjectures** that a greater number of tests would affirm a stress effect on the shape parameter. Also, as previously mentioned, non-constant shape parameter has been frequently observed in rolling contact fatigue testing. Based on this additional information, which is external to the data from the life test, one could argue that the non-constant shape parameter is feasible, especially if one were to extrapolate the results to stresses outside the available range. 

The 10% and 50% quantiles ^[Delta method was used to compute the standard errors of the quantiles.] from the varying shape parameter model and the constant shape parameter model are shown in Table 2. 

```{r Quantile Calculation, include=FALSE}
#here are the quantiles and their standard errors for the constant shape model

para_fit_weibull.Fig17.10d <- survival::survreg(Surv(Revolutions, delta) ~ log(Stress), data = ceramic, dist = "weibull")

quantiles_0.1_0.5 <- predict(para_fit_weibull.Fig17.10d,
        newdata = data.frame(Stress = 1.15), type = 'quantile', p=c(0.1,0.5), se.fit = T) 

lower_wald95 <- exp(log(quantiles_0.1_0.5$fit) - (qnorm(0.975)*quantiles_0.1_0.5$se.fit/quantiles_0.1_0.5$fit))

upper_wald95 <- exp(log(quantiles_0.1_0.5$fit) + (qnorm(0.975)*quantiles_0.1_0.5$se.fit/quantiles_0.1_0.5$fit))

#here are the quantiles and their standard errors from the varying shape parameter model. These requre the delta method to calculate. See your hand written notes.

z.10 = log(-log(1-0.1))
z.50 = log(-log(1-0.5))

quantile_0.1_varSig <- exp(beta.0 + (beta.1*log(1.15)) + (z.10*exp(beta.0s + (beta.1s*log(1.15)))))

quantile_0.5_varSig <- exp(beta.0 + (beta.1*log(1.15)) + (z.50*exp(beta.0s + (beta.1s*log(1.15)))))

#A on page 13-C in your notes

A <- varcov[1,1] + (((log(1.15))**2)*varcov[2,2]) + (2*(log(1.15))*varcov[1,2])

#B on page 14-C in your notes

B <- (((exp(beta.0s + (beta.1s*log(1.15))))**2)*varcov[3,3]) + (((log(1.15)*exp(beta.0s + (beta.1s*log(1.15))))**2)*varcov[4,4]) + ((log(1.15)*((exp(beta.0s + (beta.1s*log(1.15))))**2))*varcov[3,4])

#C on pages 16-C and 17-C

C <- ((exp(beta.0s + (beta.1s*log(1.15))))*varcov[1,3]) + (log(1.15)*(exp(beta.0s + (beta.1s*log(1.15))))*varcov[1,4]) + (log(1.15)*(exp(beta.0s + (beta.1s*log(1.15))))*varcov[2,3]) + (((log(1.15))**2)*(exp(beta.0s + (beta.1s*log(1.15))))*varcov[2,4])

Var_Log.quantile_0.1_varSig <- A + ((z.10**2)*B) + (2*z.10*C)

std.err_Log.quantile_0.1_varSig <- sqrt(Var_Log.quantile_0.1_varSig)

std.err_quantile_0.1_varSig <- quantile_0.1_varSig*std.err_Log.quantile_0.1_varSig

lower_wald95_quantile_0.1_varSig <- exp(log(quantile_0.1_varSig) - (qnorm(0.975)*std.err_Log.quantile_0.1_varSig))

upper_wald95_quantile_0.1_varSig <- exp(log(quantile_0.1_varSig) + (qnorm(0.975)*std.err_Log.quantile_0.1_varSig))

Var_Log.quantile_0.5_varSig <- A + ((z.50**2)*B) + (2*z.50*C)

std.err_Log.quantile_0.5_varSig <- sqrt(Var_Log.quantile_0.5_varSig)

std.err_quantile_0.5_varSig <- quantile_0.5_varSig*std.err_Log.quantile_0.5_varSig

lower_wald95_quantile_0.5_varSig <- exp(log(quantile_0.5_varSig) - (qnorm(0.975)*std.err_Log.quantile_0.5_varSig))

upper_wald95_quantile_0.5_varSig <- exp(log(quantile_0.5_varSig) + (qnorm(0.975)*std.err_Log.quantile_0.5_varSig))

#data frame of quantiles

quantile_df <- data.frame(t.p = c(quantiles_0.1_0.5$fit,quantile_0.1_varSig,quantile_0.5_varSig), t.p.se = c(quantiles_0.1_0.5$se.fit,std.err_quantile_0.1_varSig,std.err_quantile_0.5_varSig), wald_lower = c(lower_wald95,lower_wald95_quantile_0.1_varSig,lower_wald95_quantile_0.5_varSig),
                          wald_upper = c(upper_wald95,upper_wald95_quantile_0.1_varSig,upper_wald95_quantile_0.5_varSig))

quantile_df<- quantile_df %>%
  dplyr::mutate(quantile = c("$\\hat{t_{0.1}}$","$\\hat{t_{0.5}}$","$\\hat{t_{0.1}}$","$\\hat{t_{0.5}}$")) %>%
  dplyr::select(quantile, everything()) %>%
  dplyr::mutate(Model = c(rep("Constant Shape",2),rep("Varying Shape",2))) %>%
  dplyr::select(Model, everything())

#rownames(quantile_df) <- c("$\\hat{t_{0.1}}$","$\\hat{t_{0.5}}$","$\\hat{t_{0.1}}$","$\\hat{t_{0.5}}$")

# rownames(quantile_df) <- c("Constant Shape","Constant Shape1","Varying Shape","Varying Shape1")
colnames(quantile_df) <- c("Model","Quantile","ML estimate","Standard Error","Wald Lower 95% CI","Wald Upper 95% CI")
```


```{r ML Table 5, echo=FALSE, tab.cap="Table 5. 10% and 50% quantile at 1.15 Mpsi stress"}
t <- knitr::kable(quantile_df, align = rep('c', 5), table.envir = 'table*', escape = FALSE, digits = 3) %>%
 kableExtra::kable_styling("striped", full_width = T, position = "left") 

kableExtra::collapse_rows(t)


#, caption = "Table 2. Maximum Likelihood estimates"
```

Fig.2 shows a plot of the standardized residuals against fitted values. Compare this plot to the corresponding plot for the constant shape parameter model ^[Fig.7  in the [**previous article**](https://rpubs.com/shishir909/1251052)]. Notice that the spread of the points is much more *equal* in this plot here as compared to a model with a constant shape parameter.

```{r Std. Residuals vs Fitted Values prep, include=FALSE}
#First, add a column for the linear predictors. This is nothing but "mu + gamma.transpose*Z" for every observation.

ceramic <- ceramic %>%
  dplyr::mutate(LinearPredictor = beta.0 + (beta.1*log(Stress)))

#Next, add a column for standardized residuals. The definition of standardized residual for a log-location-scale model is given by Eq 17.12 in SMRD2

ceramic <- ceramic %>%
  dplyr::mutate(sigma.i = exp(beta.0s + (beta.1s*log(Stress)))) %>%
  dplyr::mutate(stdResid = exp((log(Revolutions)-LinearPredictor)/sigma.i)) %>%
  dplyr::mutate(stdResid.round = round(stdResid,3))
```


```{r Std Resid vs fitted values, echo=FALSE, fig.cap="Figure 2: Standardized Residuals vs Fitted Values", cache=TRUE}

ggplot(ceramic, aes(x = exp(LinearPredictor), y = stdResid, group = delta)) + geom_point() + coord_trans(x = 'log', y = 'log') + ylab("Standardized Residuals") + xlab("Fitted Values") #Matches figure 17.5a! Notice that the variability is higher at larger fitted values.


#+ geom_point(aes(shape = factor(delta, levels = c(1,0))))
```

Fig. 3 shows a Weibull plot of the standardized residuals along with the 95% confidence bands. Comparing this plot to the corresponding plot in the previous article^[Fig.8 in the [**previous article**](https://rpubs.com/shishir909/1251052)] reveals that this plot suffers from a similar deviation from linearity as did the one with the constant shape parameter model.

```{r Std. Residuals Weibull Plot prep, include=FALSE}
#Next, we will plot a Weibull plot of the standardized residuals. In order to plot a Weibull plot, I need to plot log(t.p) vs log[-log(1-p)] as per Section 6.2.4 in SMRD2. Note that (1-p) is nothing but the Kaplan Meir survival probability (because p is the CDF).

#First, I need the Kaplan Meir estimate of the CDF of the standardized residuals

fitModel1.KaplanMeir <- survival::survfit(Surv(stdResid.round,delta) ~ 1, data = ceramic)

summary(fitModel1.KaplanMeir)

#Next, I need the confidence band. I tried using the survMisc package, but the confidence bands I was getting did not make sense. The survival lower and upper bounds of the survival probability band increased at some times, which does not make sense because survival function is strictly non-increasing. The survMisc package is from 2016 and is very old. I used the km.ci package with the log transformed Equal probability band and even this gave me confidence bands where the survival function increased at some point. After reading the section on confidence bands in SMRD2 (Section 3.8 in Chapter 3), I realized that it is possible that sometimes this can happen. Don't panic! All you need to do is adjust the confidence bands as mentioned in the Section 3.8. Here is what you need to do to adjust the bands: "If the upper band is decreasing on the left, it is made flat from tL(a) to the point of the minimum. If the lower band is decreasing on the right, it is made flat from the point of maximum to tU(b) . These adjustments, if needed, give tighter, more sensible bands and have no effect on the actual coverage probability of the nonparametric simultaneous bands." This is also mentioned in SAS documentation here: "https://documentation.sas.com/doc/en/pgmsascdc/9.4_3.4/statug/statug_lifetest_details11.htm", but it references the same SMRD2 book.

#Also, the latest version of the km.ci package is from 2022 and is newer as compared to survMisc. Going forward, I should use km.ci for confidence bands and not survMisc.

#survFitObject <- survival::survfit(Surv(stdResid.round,delta) ~ 1, data = ceramic) #Required for km.ci on the next line.

confBands2 <- km.ci::km.ci(fitModel1.KaplanMeir, conf.level = 0.95, method = "logep") #"logep" recommended by km.ci package documentation.

#create a dataframe with all the probabilities that I need to plot

weibullPlot.model1 <- data.frame(t.p = fitModel1.KaplanMeir[["time"]],
                          survival.p = fitModel1.KaplanMeir[["surv"]]) %>%
  dplyr::mutate(p = 1 - survival.p) %>%
  dplyr::filter(row_number() <= n()-1) %>% #The observation with survival probability 0 i.e. the last observation is removed since we cannot have confidence band values for this observation.
  dplyr::mutate(survival.lower = confBands2$lower,
                survival.upper = confBands2$upper) %>%
  dplyr::mutate(p.lower = round(1 - survival.upper,3),
                p.upper = round(1 - survival.lower,3)) 

xTransformer <- scales::trans_new(
  name = "weibull.x.transform",
  transform = function(t.p){
    x = log(t.p)
    return(x)
  },
  inverse = function(x) {
    t.p = exp(x)
    return(t.p)
  }
)


yTransfomer <- scales::trans_new(
  name = "weibull.y.transform",
  transform = function(p){
    y = log(-log(1-p))
    return(y)
  },
  inverse = function(y) {
    p = 1 - exp(-exp(y))
    return(p)
  }
)

#Plot Fig 17.5 b

ggplot(weibullPlot.model1, aes(x = t.p, y = p)) + geom_point() + geom_step(aes(y = p.lower)) + geom_step(aes(y = p.upper)) + scale_x_continuous(transform = xTransformer, name = "Standardized Residuals", breaks = c(0.001,0.01, 0.1, 1)) + scale_y_continuous(transform = yTransfomer, name = "Probability",breaks = seq(0.01,0.99,0.05))

#Notice that the upper bound of the confidence band decreases at a point. Hence, an adjustment, as discussed above, is needed.

weibullPlot.model1$p.upper[c(1)] <- weibullPlot.model1$p.upper[2]

#Plot 17.5b again, with the adjusted upper band.



```



```{r Std Resid Weibull plot, echo=FALSE, fig.cap="Figure 3: Weibull Probability Plot of Standardized Residuals", cache=TRUE}
std.resid.prob.plot <- ggplot(weibullPlot.model1, aes(x = t.p, y = p)) + geom_point() + geom_step(aes(y = p.lower)) + geom_step(aes(y = p.upper)) + scale_x_continuous(transform = xTransformer, name = "Standardized Residuals", breaks = c(0.001,0.01, 0.1, 1,2,3)) + scale_y_continuous(transform = yTransfomer, name = "Probability",breaks = seq(0.01,0.99,0.1))

R <- suppressWarnings(print(std.resid.prob.plot))

```

# Conclusion

A Weibull regression model with a varying shape parameter was fit to data from a ceramic bearings life test. A likelihood ratio test comparing this model to the constant shape parameter model gave a non-significant result, which means that there is no evidence of any improvement gained with the non-constant shape parameter model, at the 5% significance level. Nevertheless, considering that a stress effect on the shape parameter has been frequently observed in rolling contact fatigue testing, this model still has some value, especially if we need to extrapolate outside the range of stress values available outside the life test.     The residual probability plot shows deviation from linearity, which is an indication that the Weibull distribution assumption is also suspect. Additional investigation could include trying a Box-Cox transformation of the independent variable or trying a different distribution (eg: log-normal). 

# Acknowledgements

1. Most of what I have learnt about time-to-event analysis is from the book Survival Analysis: Techniques for Censored and Truncated Data, Second Edition (John P. Klein and Melvin L. Moescheberger) and Statistical Methods for Reliability Data, Second Edition (William Q. Meeker, Luis A. Escobar, Francis G. Pascual).

2. All analyses were performed using R Statistical Software(v4.4.1; R Core Team 2024). The *survival* R package(v3.7.0; Therneau T 2024) was extensively used. Please refer to next section for all the packages used, their versions and the names of the package developers.

3. This article's format is a style that Edward Tufte uses in his books and handouts. 

# Credits for R packages used

```{r echo=FALSE}
report::cite_packages()
```

# End

I hope you found this article informative! If you have any comments or suggestions or if you find any errors and are keen to help correct the error, please write to me at shishir909@gmail.com.

```{r flexsurv, include=FALSE}

fs1 <- flexsurv::flexsurvreg(Surv(Revolutions, delta) ~ log(Stress) + shape(log(Stress)), data = ceramic, dist = "weibull")

fs1

fs_quantile <- predict(fs1,
        newdata = data.frame(Stress = 1.15), type = 'quantile', p=c(0.1,0.5), se.fit = T)

#flexsurv output matches my manual calculation output!! Here is the conversion for the different parameterizations: https://drive.google.com/open?id=1-j9vebU_xxdKl4vMuT5Ph2wEZqlkI632&usp=drive_fs

```




```{r bib, include=FALSE}
# create a bib file for the R packages used in this document
knitr::write_bib(c('base', 'rmarkdown'), file = 'skeleton.bib')
```
