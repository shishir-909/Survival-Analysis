) + scale_y_continuous(
transform = yTransfomer.weibull,
name = "Fraction Failing",
breaks = seq(0.0,1,0.1)
) + ggtitle(label = "Weibull Probability Plot")
#log-normal plot
fit.logNormal <- survival::survreg(Surv(Operations, delta) ~ 1, data = MechanicalSwitch, dist = "lognormal")
#summary(fit.logNormal)
mu.ML <- fit.logNormal$coefficients
sigma.ML <- fit.logNormal$scale
#Points (We already have this from the Weibull plot)
#Line. Note that lognormal ML fit already available from fit.logNormal
mu.ML_logNormal <- fit.logNormal$coefficients
se_mu.ML_logNormal <- sqrt(fit.logNormal[["var"]][1,1])
sigma.ML_logNormal <- fit.logNormal$scale
se_sigma.ML_logNormal <- sqrt((sigma.ML_logNormal**2)*fit.logNormal[["var"]][2,2])
#Dashed lines. Prepare a dataframe to hold values of the CI for F(t). We will use the CI based on "e", as done in chunk 5
t <-seq(1.1,3.8,length = 10000) #Range of data used
e <- (log(t) - mu.ML_logNormal)/sigma.ML_logNormal
Ft.ML_Lognormal <- pnorm(e)
#To get standard error of the above CDF, use equation 8.8 from SMRD2. My proof for 8.8 is here: https://drive.google.com/open?id=12D9tcASJAgxY2kecw2tbCGUK9a8RfrQK&usp=drive_fs
dgdMu <- (-1/sigma.ML_logNormal)*dnorm(e)
dgdSigma <- (-e/sigma.ML_logNormal)*dnorm(e)
se_Ft.ML_logNormal <- sqrt(((dgdMu**2)*(se_mu.ML_logNormal**2)) + (2*(dgdMu)*(dgdSigma)*(sigma.ML_logNormal*fit.logNormal[["var"]][1,2])) + ((dgdSigma**2)*(se_sigma.ML_logNormal**2)))
#Confidence interval based on "e".
se_e_logNormal <- (1/sigma.ML_logNormal)*sqrt((se_mu.ML_logNormal**2) + (2*e*(sigma.ML_logNormal*fit.logNormal[["var"]][1,2])) + ((e**2)*(se_sigma.ML_logNormal**2)))
CI_lower_e_logNormal <- e - (qnorm(0.975)*se_e_logNormal)
CI_upper_e_logNormal <- e + (qnorm(0.975)*se_e_logNormal)
CI_lower_eFt_logNormal <- pnorm(CI_lower_e_logNormal)
CI_upper_eFt_logNormal <- pnorm(CI_upper_e_logNormal)
CDF_CI_logNormal_df <- data.frame(Operations = t,
CI_Lower = CI_lower_eFt_logNormal,
CI_Upper = CI_upper_eFt_logNormal)
#Transformers
xTransformer.logNormal <- scales::trans_new(
name = "logNormal.x.transform",
transform = function(t.p){
x = log(t.p)
return(x)
},
inverse = function(x) {
t.p = exp(x)
return(t.p)
}
)
yTransfomer.logNormal <- scales::trans_new(
name = "logNormal.y.transform",
transform = function(p){
y = qnorm(p)
return(y)
},
inverse = function(y) {
p = pnorm(y)
return(p)
}
)
gg.logNormal <- ggplot() + geom_point(data = CDF_df, aes(x = Operations, y = p)) + geom_line(data = CDF_CI_logNormal_df,
aes(x = Operations, y = CI_Lower),
linetype = "dashed") + geom_line(data= CDF_CI_logNormal_df, aes(x = Operations, y = CI_Upper), linetype = "dashed") + geom_abline(
aes(intercept = (-fit.logNormal$coefficients/fit.logNormal$scale), #y-axis intercept
slope = 1/fit.logNormal$scale)
)  + scale_x_continuous(
transform = xTransformer.logNormal,
name = "Millions of Operations",
breaks = seq(1,4, by = 0.2)
) + scale_y_continuous(
transform = yTransfomer.logNormal,
name = "Fraction Failing",
breaks = seq(0.01,0.99,0.05)
) + ggtitle(label = "logNormal Probability Plot")
gridExtra::grid.arrange(gg.weibull, gg.logNormal, ncol = 2)
gg.weibull + gg.logNormal + facet_wrap(~plot, ncol = 2)
df <- gg.weibull + gg.logNormal + facet_wrap(~plot, ncol = 2)
AIC.weibull <- AIC(fit.weibull)
AIC.lognormal <- AIC(fit.logNormal)
# Chunk 1: setup
library(tufte)
library(knitr)
library(kableExtra)
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(cache.extra = packageVersion("tufte"), echo = FALSE)
options(htmltools.dir.version = FALSE)
library(survival)
library(tidyverse)
library(readr)
setwd("G:/My Drive/rao@ualberta.ca 2022-12-08 10 58/shishir@tamu.edu/My Drive/Interesting papers/Survival Models/GitHub/Survival/Survival-Analysis/Blog 9")
# Chunk 2: Load Table
MechanicalSwitch <- readr::read_csv("Data\\MechanicalSwitch.csv", show_col_types = FALSE)
knitr::kable(MechanicalSwitch,
align = rep('c', 5),
table.envir = 'table*') %>%
kableExtra::kable_styling("striped", full_width = T, position = "left") %>%
kableExtra::scroll_box(height = "200px")
# Chunk 3: Weibull plot
MechanicalSwitch <- MechanicalSwitch %>%
dplyr::mutate(delta = ifelse(MechanicalSwitch$`Failure Mode` == "Censored",0,1)) %>%
dplyr::rename(Operations = `Millions of Operations`)
# Weibull plot
#Points
fit_3 <- survival::survfit(Surv(Operations, delta) ~ 1, data = MechanicalSwitch)
#summary(fit_3)
CDF_df <- data.frame(Operations = fit_3$time,
p = 1 - fit_3$surv) %>%
dplyr::distinct(p, .keep_all = TRUE) #This line is required. Otherwise, the time column includes all observations. I just need the observations where there is a jump in the CDF.
CDF_df <- CDF_df %>%
dplyr::mutate(bottomOfStairs = c(0,head(p,-1))) %>%
dplyr::mutate(middleOfStairs = (p + bottomOfStairs)/2) %>% #Note: p is top of stairs.
dplyr::select(!p) %>%
dplyr::rename(p = middleOfStairs) #This piece of code is for plotting the CDF at the mid point of the jumps instead of plotting at the top of the jumps. See Section 6.4.2 of Chapter 6 in SMRD2.
#Line.
fit.weibull <- survival::survreg(Surv(Operations, delta) ~ 1, data = MechanicalSwitch, dist = "weibull")
#summary(fit.weibull)
mu.ML_Weibull <- fit.weibull$coefficients
se_mu.ML_Weibull <- sqrt(fit.weibull[["var"]][1,1])
sigma.ML_Weibull <- fit.weibull$scale
se_sigma.ML_Weibull <- sqrt((sigma.ML_Weibull**2)*fit.weibull[["var"]][2,2])
AIC.weibull <- round(AIC(fit.weibull),2)
#Dashed lines. Prepare a dataframe to hold values of the CI for F(t). We will use the CI based on "e", as done in chunk 4
t <-seq(1.1,3.8,length = 10000) #Range of data used
e <- (log(t) - mu.ML_Weibull)/sigma.ML_Weibull
dgdMu <- (-1/sigma.ML_Weibull)*(exp(-exp(e)))*(exp(e))
dgdSigma <- (-e/sigma.ML_Weibull)*(exp(-exp(e)))*(exp(e))
se_Ft.ML_Weibull <- sqrt(((dgdMu**2)*(se_mu.ML_Weibull**2)) + (2*(dgdMu)*(dgdSigma)*(sigma.ML_Weibull*fit.weibull[["var"]][1,2])) + ((dgdSigma**2)*(se_sigma.ML_Weibull**2)))
#Confidence interval based on "e".
se_e_Weibull <- (1/sigma.ML_Weibull)*sqrt((se_mu.ML_Weibull**2) + (2*e*(sigma.ML_Weibull*fit.weibull[["var"]][1,2])) + ((e**2)*(se_sigma.ML_Weibull**2)))
CI_lower_e_Weibull <- e - (qnorm(0.975)*se_e_Weibull)
CI_upper_e_Weibull <- e + (qnorm(0.975)*se_e_Weibull)
CI_lower_eFt_Weibull <- 1 - exp(-exp(CI_lower_e_Weibull))
CI_upper_eFt_Weibull <- 1 - exp(-exp(CI_upper_e_Weibull))
CDF_CI_Weibull_df <- data.frame(Operations = t,
CI_Lower = CI_lower_eFt_Weibull,
CI_Upper = CI_upper_eFt_Weibull)
#Transformers
xTransformer.weibull <- scales::trans_new(
name = "weibull.x.transform",
transform = function(t.p){
x = log(t.p)
return(x)
},
inverse = function(x) {
t.p = exp(x)
return(t.p)
}
)
yTransfomer.weibull <- scales::trans_new(
name = "weibull.y.transform",
transform = function(p){
y = log(-log(1-p))
return(y)
},
inverse = function(y) {
p = 1 - exp(-exp(y))
return(p)
}
)
gg.weibull <- ggplot() + geom_point(data = CDF_df, aes(x = Operations, y = p)) + geom_line(data = CDF_CI_Weibull_df,
aes(x = Operations, y = CI_Lower),
linetype = "dashed") + geom_line(data= CDF_CI_Weibull_df, aes(x = Operations, y = CI_Upper), linetype = "dashed") + geom_abline(
aes(intercept = (-fit.weibull$coefficients/fit.weibull$scale), #y-axis intercept
slope = 1/fit.weibull$scale)
)  + scale_x_continuous(
transform = xTransformer.weibull,
name = "Millions of Operations",
breaks = seq(1,4, by = 0.5)
) + scale_y_continuous(
transform = yTransfomer.weibull,
name = "Fraction Failing",
breaks = seq(0.0,1,0.1)
) #+ ggtitle(label = "Weibull Probability Plot")
gg.weibull
# Chunk 4: log-normal plot
#log-normal plot
fit.logNormal <- survival::survreg(Surv(Operations, delta) ~ 1, data = MechanicalSwitch, dist = "lognormal")
#summary(fit.logNormal)
mu.ML <- fit.logNormal$coefficients
sigma.ML <- fit.logNormal$scale
AIC.lognormal <- round(AIC(fit.logNormal),2)
#Points (We already have this from the Weibull plot)
#Line. Note that lognormal ML fit already available from fit.logNormal
mu.ML_logNormal <- fit.logNormal$coefficients
se_mu.ML_logNormal <- sqrt(fit.logNormal[["var"]][1,1])
sigma.ML_logNormal <- fit.logNormal$scale
se_sigma.ML_logNormal <- sqrt((sigma.ML_logNormal**2)*fit.logNormal[["var"]][2,2])
#Dashed lines. Prepare a dataframe to hold values of the CI for F(t). We will use the CI based on "e", as done in chunk 5
t <-seq(1.1,3.8,length = 10000) #Range of data used
e <- (log(t) - mu.ML_logNormal)/sigma.ML_logNormal
Ft.ML_Lognormal <- pnorm(e)
#To get standard error of the above CDF, use equation 8.8 from SMRD2. My proof for 8.8 is here: https://drive.google.com/open?id=12D9tcASJAgxY2kecw2tbCGUK9a8RfrQK&usp=drive_fs
dgdMu <- (-1/sigma.ML_logNormal)*dnorm(e)
dgdSigma <- (-e/sigma.ML_logNormal)*dnorm(e)
se_Ft.ML_logNormal <- sqrt(((dgdMu**2)*(se_mu.ML_logNormal**2)) + (2*(dgdMu)*(dgdSigma)*(sigma.ML_logNormal*fit.logNormal[["var"]][1,2])) + ((dgdSigma**2)*(se_sigma.ML_logNormal**2)))
#Confidence interval based on "e".
se_e_logNormal <- (1/sigma.ML_logNormal)*sqrt((se_mu.ML_logNormal**2) + (2*e*(sigma.ML_logNormal*fit.logNormal[["var"]][1,2])) + ((e**2)*(se_sigma.ML_logNormal**2)))
CI_lower_e_logNormal <- e - (qnorm(0.975)*se_e_logNormal)
CI_upper_e_logNormal <- e + (qnorm(0.975)*se_e_logNormal)
CI_lower_eFt_logNormal <- pnorm(CI_lower_e_logNormal)
CI_upper_eFt_logNormal <- pnorm(CI_upper_e_logNormal)
CDF_CI_logNormal_df <- data.frame(Operations = t,
CI_Lower = CI_lower_eFt_logNormal,
CI_Upper = CI_upper_eFt_logNormal)
#Transformers
xTransformer.logNormal <- scales::trans_new(
name = "logNormal.x.transform",
transform = function(t.p){
x = log(t.p)
return(x)
},
inverse = function(x) {
t.p = exp(x)
return(t.p)
}
)
yTransfomer.logNormal <- scales::trans_new(
name = "logNormal.y.transform",
transform = function(p){
y = qnorm(p)
return(y)
},
inverse = function(y) {
p = pnorm(y)
return(p)
}
)
gg.logNormal <- ggplot() + geom_point(data = CDF_df, aes(x = Operations, y = p)) + geom_line(data = CDF_CI_logNormal_df,
aes(x = Operations, y = CI_Lower),
linetype = "dashed") + geom_line(data= CDF_CI_logNormal_df, aes(x = Operations, y = CI_Upper), linetype = "dashed") + geom_abline(
aes(intercept = (-fit.logNormal$coefficients/fit.logNormal$scale), #y-axis intercept
slope = 1/fit.logNormal$scale)
)  + scale_x_continuous(
transform = xTransformer.logNormal,
name = "Millions of Operations",
breaks = seq(1,4, by = 0.5)
) + scale_y_continuous(
transform = yTransfomer.logNormal,
name = "Fraction Failing",
breaks = seq(0.0,1,0.1)
) #+ ggtitle(label = "log-normal Probability Plot")
gg.logNormal
#gridExtra::grid.arrange(gg.weibull, gg.logNormal, ncol = 2)
# Chunk 5
# Fit a log-normal distribution
fit.logNormal <- survival::survreg(Surv(Operations, delta) ~ 1, data = MechanicalSwitch, dist = "lognormal")
summary(fit.logNormal)
mu.ML <- fit.logNormal$coefficients
sigma.ML <- fit.logNormal$scale
ML_estimates <- data.frame(c(mu.ML_logNormal,
se_mu.ML_logNormal,
sigma.ML_logNormal,
se_sigma.ML_logNormal))
rownames(ML_estimates) <- c("$\\hat{\\mu}$","$\\mathrm SE_{\\hat{\\mu}}$","$\\hat{\\sigma}$","$\\mathrm SE_{\\hat{\\sigma}}$")
knitr::kable(ML_estimates, align = rep('c', 1), table.envir = 'table*', escape = FALSE, digits = 3) %>%
kableExtra::kable_styling("striped", full_width = T, position = "left")
ML_estimates <- data.frame(c(mu.ML_logNormal,
se_mu.ML_logNormal,
sigma.ML_logNormal,
se_sigma.ML_logNormal))
rownames(ML_estimates) <- c("$\\hat{\\mu}$","$\\mathrm SE_{\\hat{\\mu}}$","$\\hat{\\sigma}$","$\\mathrm SE_{\\hat{\\sigma}}$")
colnames(ML_estimates) <- c(,"Value")
rownames(ML_estimates) <- c("$\\hat{\\mu}$","$\\mathrm SE_{\\hat{\\mu}}$","$\\hat{\\sigma}$","$\\mathrm SE_{\\hat{\\sigma}}$")
colnames(ML_estimates) <- c("Parameter","Estimate")
colnames(ML_estimates) <- c("Estimate")
knitr::kable(ML_estimates, align = rep('c', 1), table.envir = 'table*', escape = FALSE, digits = 3) %>%
kableExtra::kable_styling("striped", full_width = T, position = "left")
knitr::kable(ML_estimates, align = rep('c', 1), table.envir = 'table*', escape = FALSE, digits = 3) %>%
kableExtra::kable_styling("striped", full_width = F, position = "left")
T.tilda.lower <- exp(mu.ML + (qnorm(0.05)*sigma.ML))
T.tilda.upper <- exp(mu.ML + (qnorm(0.95)*sigma.ML))
#The 2 sided 90% prediction interval is (T.tilda.lower, T.tilda.upper)
T.tilda.lower <- round(exp(mu.ML + (qnorm(0.05)*sigma.ML)),2)
T.tilda.upper <- round(exp(mu.ML + (qnorm(0.95)*sigma.ML)),2)
#The 2 sided 90% prediction interval is (T.tilda.lower, T.tilda.upper)
alpha.c <- 1 - seq(0.93,0.995, by = 0.05)
alpha.c <- 1 - seq(0.93,0.995, by = 0.005)
alpha.c <- 1 - seq(0.93,0.995, by = 0.0005)
alpha.c <- 1 - seq(0.93,0.995, by = 0.005)
start.time <- Sys.time()
B <- 10000
n <- nrow(MechanicalSwitch)
r <- sum(MechanicalSwitch$delta)
mu.B <- c()
sigma.B <- c()
alpha.c <- 1 - seq(0.93,0.995, by = 0.005)
#alpha.c <- 1 - 0.96 #start with alpha.c as "1 - 0.95". Then, play around with it to find a value that gives CP.PI.PB.upper as 0.95. Then repeat process again to find a value that gives CP.PI.FRW.lower as 0.95. The formula for CP.PI.FRW.upper and CP.PI.PB.lower is given below.
j <- 1
CP.PI.FRW.upper <- c()
CP.PI.FRW.lower <- c()
while(j <= length(alpha.c)){
i <- 1
while(i <= B){
if(i%%1000 == 0){print(i)}
#The next 3 lines of code are for generating the fractional random weights for simulation.
gamma_randomNumbers <- rgamma(n, shape = 1, rate = 1)
dirichlet_randomNumbers <- gamma_randomNumbers/sum(gamma_randomNumbers)
fractional_randomWeights <- n*dirichlet_randomNumbers
fit_B <- survival::survreg(Surv(Operations, delta) ~ 1, data = MechanicalSwitch, weights = fractional_randomWeights,dist = "lognormal")
mu.B[i] <- fit_B$coefficients
sigma.B[i] <- fit_B$scale
i <- i + 1
}
T.tilda.lower.B <- exp(mu.B + (qnorm(alpha.c[j])*sigma.B))
T.tilda.upper.B <- exp(mu.B + (qnorm(1-alpha.c[j])*sigma.B))
#The next 2 lines of code gives the coverage probability for the upper and lower bounds. See section 3.3 in the paper linked here: https://drive.google.com/open?id=1yqxRGOqa1-FVEou9B8jw89xQSjmYPiij&usp=drive_fs
(CP.PI.FRW.upper[j] <- ((1/B)*sum(pnorm((log(T.tilda.upper.B)-mu.ML)/sigma.ML))))
(CP.PI.FRW.lower[j] <- ((1/B)*sum(1-pnorm((log(T.tilda.lower.B)-mu.ML)/sigma.ML))))
j <- j + 1
}
end.time <- Sys.time()
end.time - start.time
CP.PI.FRW.upper
1 - alpha.c
calibrated.probs.df <- data.frame(`1-alpha.c` = 1 - alpha.c,
CP.PI.FRW.lower = CP.PI.FRW.lower,
CP.PI.FRW.upper = CP.PI.FRW.upper)
ggplot() + geom_line(data = calibrated.probs.df,
aes(x = `1-alpha.c`, y = CP.PI.FRW.lower)) + geom_line(data = calibrated.probs.df,
aes(x = `1-alpha.c`, y = CP.PI.FRW.upper))
View(calibrated.probs.df)
calibrated.probs.df <- data.frame(oneMinusAlphac = 1 - alpha.c,
CP.PI.FRW.lower = CP.PI.FRW.lower,
CP.PI.FRW.upper = CP.PI.FRW.upper)
ggplot() + geom_line(data = calibrated.probs.df,
aes(x = oneMinusAlphac, y = CP.PI.FRW.lower)) + geom_line(data = calibrated.probs.df,
aes(x = oneMinusAlphac, y = CP.PI.FRW.upper))
alpha.c <- 1 - seq(0.93,0.995, by = 0.001)
i=1
j=1
print(c(j.i))
cat("j = ",j,"\ni",i)
cat("j = ",j," & i =",i)
start.time <- Sys.time()
B <- 10000
n <- nrow(MechanicalSwitch)
r <- sum(MechanicalSwitch$delta)
mu.B <- c()
sigma.B <- c()
alpha.c <- 1 - seq(0.93,0.995, by = 0.001)
#alpha.c <- 1 - 0.96 #start with alpha.c as "1 - 0.95". Then, play around with it to find a value that gives CP.PI.PB.upper as 0.95. Then repeat process again to find a value that gives CP.PI.FRW.lower as 0.95. The formula for CP.PI.FRW.upper and CP.PI.PB.lower is given below.
j <- 1
CP.PI.FRW.upper <- c()
CP.PI.FRW.lower <- c()
while(j <= length(alpha.c)){
i <- 1
while(i <= B){
if(i%%1000 == 0){#print(i)
cat("j = ",j," & i =",i)}
#The next 3 lines of code are for generating the fractional random weights for simulation.
gamma_randomNumbers <- rgamma(n, shape = 1, rate = 1)
dirichlet_randomNumbers <- gamma_randomNumbers/sum(gamma_randomNumbers)
fractional_randomWeights <- n*dirichlet_randomNumbers
fit_B <- survival::survreg(Surv(Operations, delta) ~ 1, data = MechanicalSwitch, weights = fractional_randomWeights,dist = "lognormal")
mu.B[i] <- fit_B$coefficients
sigma.B[i] <- fit_B$scale
i <- i + 1
}
T.tilda.lower.B <- exp(mu.B + (qnorm(alpha.c[j])*sigma.B))
T.tilda.upper.B <- exp(mu.B + (qnorm(1-alpha.c[j])*sigma.B))
#The next 2 lines of code gives the coverage probability for the upper and lower bounds. See section 3.3 in the paper linked here: https://drive.google.com/open?id=1yqxRGOqa1-FVEou9B8jw89xQSjmYPiij&usp=drive_fs
(CP.PI.FRW.upper[j] <- ((1/B)*sum(pnorm((log(T.tilda.upper.B)-mu.ML)/sigma.ML))))
(CP.PI.FRW.lower[j] <- ((1/B)*sum(1-pnorm((log(T.tilda.lower.B)-mu.ML)/sigma.ML))))
j <- j + 1
}
start.time <- Sys.time()
B <- 10000
n <- nrow(MechanicalSwitch)
r <- sum(MechanicalSwitch$delta)
mu.B <- c()
sigma.B <- c()
alpha.c <- 1 - seq(0.93,0.995, by = 0.001)
#alpha.c <- 1 - 0.96 #start with alpha.c as "1 - 0.95". Then, play around with it to find a value that gives CP.PI.PB.upper as 0.95. Then repeat process again to find a value that gives CP.PI.FRW.lower as 0.95. The formula for CP.PI.FRW.upper and CP.PI.PB.lower is given below.
j <- 1
CP.PI.FRW.upper <- c()
CP.PI.FRW.lower <- c()
while(j <= length(alpha.c)){
i <- 1
while(i <= B){
if(i%%1000 == 0){#print(i)
cat("\nj = ",j," & i =",i)}
#The next 3 lines of code are for generating the fractional random weights for simulation.
gamma_randomNumbers <- rgamma(n, shape = 1, rate = 1)
dirichlet_randomNumbers <- gamma_randomNumbers/sum(gamma_randomNumbers)
fractional_randomWeights <- n*dirichlet_randomNumbers
fit_B <- survival::survreg(Surv(Operations, delta) ~ 1, data = MechanicalSwitch, weights = fractional_randomWeights,dist = "lognormal")
mu.B[i] <- fit_B$coefficients
sigma.B[i] <- fit_B$scale
i <- i + 1
}
T.tilda.lower.B <- exp(mu.B + (qnorm(alpha.c[j])*sigma.B))
T.tilda.upper.B <- exp(mu.B + (qnorm(1-alpha.c[j])*sigma.B))
#The next 2 lines of code gives the coverage probability for the upper and lower bounds. See section 3.3 in the paper linked here: https://drive.google.com/open?id=1yqxRGOqa1-FVEou9B8jw89xQSjmYPiij&usp=drive_fs
(CP.PI.FRW.upper[j] <- ((1/B)*sum(pnorm((log(T.tilda.upper.B)-mu.ML)/sigma.ML))))
(CP.PI.FRW.lower[j] <- ((1/B)*sum(1-pnorm((log(T.tilda.lower.B)-mu.ML)/sigma.ML))))
j <- j + 1
}
CP.PI.FRW.upper
calibrated.probs.df <- data.frame(oneMinusAlphac = 1 - alpha.c,
CP.PI.FRW.lower = CP.PI.FRW.lower,
CP.PI.FRW.upper = CP.PI.FRW.upper)
ggplot() + geom_line(data = calibrated.probs.df,
aes(x = oneMinusAlphac, y = CP.PI.FRW.lower), col = "blue") + geom_line(data = calibrated.probs.df,
aes(x = oneMinusAlphac, y = CP.PI.FRW.upper), col = "red")
end.time <- Sys.time()
end.time - start.time
View(calibrated.probs.df)
start.time <- Sys.time()
B <- 10000
n <- nrow(MechanicalSwitch)
r <- sum(MechanicalSwitch$delta)
mu.B <- c()
sigma.B <- c()
alpha.c <- 1 - 0.95 #start with alpha.c as "1 - 0.95". Then, play around with it to find a value that gives CP.PI.PB.upper as 0.95. Then repeat process again to find a value that gives CP.PI.FRW.lower as 0.95. The formula for CP.PI.FRW.upper and CP.PI.PB.lower is given below.
i <- 1
while(i <= B){
if(i%%1000 == 0){print(i)}
#The next 3 lines of code are for generating the fractional random weights for simulation.
gamma_randomNumbers <- rgamma(n, shape = 1, rate = 1)
dirichlet_randomNumbers <- gamma_randomNumbers/sum(gamma_randomNumbers)
fractional_randomWeights <- n*dirichlet_randomNumbers
fit_B <- survival::survreg(Surv(Operations, delta) ~ 1, data = MechanicalSwitch, weights = fractional_randomWeights,dist = "lognormal")
mu.B[i] <- fit_B$coefficients
sigma.B[i] <- fit_B$scale
i <- i + 1
}
T.tilda.lower.B <- exp(mu.B + (qnorm(alpha.c)*sigma.B))
T.tilda.upper.B <- exp(mu.B + (qnorm(1-alpha.c)*sigma.B))
#The next 2 lines of code gives the coverage probability for the upper and lower bounds. See section 3.3 in the paper linked here: https://drive.google.com/open?id=1yqxRGOqa1-FVEou9B8jw89xQSjmYPiij&usp=drive_fs
(CP.PI.FRW.upper <- ((1/B)*sum(pnorm((log(T.tilda.upper.B)-mu.ML)/sigma.ML))))
(CP.PI.FRW.lower <- ((1/B)*sum(1-pnorm((log(T.tilda.lower.B)-mu.ML)/sigma.ML))))
#The calibrated values of the prediction bounds/interval shown below. This depends on the value of alpha.c that gives approx 95% coverage probability. This will be different for the upper and lower bounds. You will have to play around with it. See page 2 in the link here: https://drive.google.com/open?id=1Lf45d_og0teSJCYXarHQp5rxaCOofnSc&usp=drive_fs
(T.tilda.lower.calib.FRW <- exp(mu.ML + (qnorm(1-0.96)*sigma.ML)))
(T.tilda.upper.calib.FRW <- exp(mu.ML + (qnorm(0.96)*sigma.ML)))
end.time <- Sys.time()
end.time - start.time
start.time <- Sys.time()
B <- 10000
n <- nrow(MechanicalSwitch)
r <- sum(MechanicalSwitch$delta)
mu.B <- c()
sigma.B <- c()
alpha.c <- 1 - 0.96 #start with alpha.c as "1 - 0.95". Then, play around with it to find a value that gives CP.PI.PB.upper as 0.95. Then repeat process again to find a value that gives CP.PI.FRW.lower as 0.95. The formula for CP.PI.FRW.upper and CP.PI.PB.lower is given below.
i <- 1
while(i <= B){
if(i%%1000 == 0){print(i)}
#The next 3 lines of code are for generating the fractional random weights for simulation.
gamma_randomNumbers <- rgamma(n, shape = 1, rate = 1)
dirichlet_randomNumbers <- gamma_randomNumbers/sum(gamma_randomNumbers)
fractional_randomWeights <- n*dirichlet_randomNumbers
fit_B <- survival::survreg(Surv(Operations, delta) ~ 1, data = MechanicalSwitch, weights = fractional_randomWeights,dist = "lognormal")
mu.B[i] <- fit_B$coefficients
sigma.B[i] <- fit_B$scale
i <- i + 1
}
T.tilda.lower.B <- exp(mu.B + (qnorm(alpha.c)*sigma.B))
T.tilda.upper.B <- exp(mu.B + (qnorm(1-alpha.c)*sigma.B))
#The next 2 lines of code gives the coverage probability for the upper and lower bounds. See section 3.3 in the paper linked here: https://drive.google.com/open?id=1yqxRGOqa1-FVEou9B8jw89xQSjmYPiij&usp=drive_fs
(CP.PI.FRW.upper <- ((1/B)*sum(pnorm((log(T.tilda.upper.B)-mu.ML)/sigma.ML))))
(CP.PI.FRW.lower <- ((1/B)*sum(1-pnorm((log(T.tilda.lower.B)-mu.ML)/sigma.ML))))
#The calibrated values of the prediction bounds/interval shown below. This depends on the value of alpha.c that gives approx 95% coverage probability. This will be different for the upper and lower bounds. You will have to play around with it. See page 2 in the link here: https://drive.google.com/open?id=1Lf45d_og0teSJCYXarHQp5rxaCOofnSc&usp=drive_fs
(T.tilda.lower.calib.FRW <- exp(mu.ML + (qnorm(1-0.96)*sigma.ML)))
(T.tilda.upper.calib.FRW <- exp(mu.ML + (qnorm(0.96)*sigma.ML)))
end.time <- Sys.time()
end.time - start.time
T.tilda.lower.calib.FRW <- round(exp(mu.ML + (qnorm(1-0.96)*sigma.ML)),2)
T.tilda.upper.calib.FRW <- round(exp(mu.ML + (qnorm(0.96)*sigma.ML)),2)
pred.int.df <- data.frame(naive = cat("[",T.tilda.lower,", ",T.tilda.upper,"]"),
calibrated = cat("[",T.tilda.lower.calib.FRW,", ",T.tilda.upper.calib.FRW,"]"))
View(pred.int.df)
pred.int.df <- data.frame(naive = cat("[",T.tilda.lower,",",T.tilda.upper,"]"),
calibrated = cat("[",T.tilda.lower.calib.FRW,",",T.tilda.upper.calib.FRW,"]"))
colnames(ML_estimates) <- c("Naive 90% Prediction Interval", "Calibrated 90% Prediction Interval")
colnames(pred.int.df) <- c("Naive 90% Prediction Interval", "Calibrated 90% Prediction Interval")
View(pred.int.df)
pred.int.df <- data.frame(naive = cat("[",T.tilda.lower,",",T.tilda.upper,"]"),
calibrated = cat("[",T.tilda.lower.calib.FRW,",",T.tilda.upper.calib.FRW,"]"))
pred.int.df <- data.frame(naive = "a",
calibrated = "b")
View(pred.int.df)
pred.int.df$naive[1] <- cat("[",T.tilda.lower,",",T.tilda.upper,"]")
pred.int.df$naive[1] <- as.char(cat("[",T.tilda.lower,",",T.tilda.upper,"]"))
pred.int.df$naive[1] <- as.character(cat("[",T.tilda.lower,",",T.tilda.upper,"]"))
pred.int.df$naive[1]
as.character(cat("[",T.tilda.lower,",",T.tilda.upper,"]"))
pred.int.df$naive[1] <- as.character((cat("[",T.tilda.lower,",",T.tilda.upper,"]")))
length(pred.int.df$naive)
pred.int.df$naive <- as.character((cat("[",T.tilda.lower,",",T.tilda.upper,"]")))
pred.int.df$naive <- c(as.character((cat("[",T.tilda.lower,",",T.tilda.upper,"]"))))
pred.int.df$naive[1] <- c(as.character((cat("[",T.tilda.lower,",",T.tilda.upper,"]"))))
length(as.character((cat("[",T.tilda.lower,",",T.tilda.upper,"]"))))
typeof(cat("[",T.tilda.lower,",",T.tilda.upper,"]"))
typeof(as.character(cat("[",T.tilda.lower,",",T.tilda.upper,"]")))
(cat("[",T.tilda.lower,",",T.tilda.upper,"]"))
pred.int.df$naive[1] <- "(cat("[",T.tilda.lower,",",T.tilda.upper,"]"))"
(paste0("[",T.tilda.lower,",",T.tilda.upper,"]"))
pred.int.df <- data.frame(naive = "a",
calibrated = "b")
pred.int.df$naive[1] <- (paste0("[",T.tilda.lower,",",T.tilda.upper,"]"))
pred.int.df <- data.frame(naive = paste0("[",T.tilda.lower,",",T.tilda.upper,"]"),
calibrated = paste0("[",T.tilda.lower.calib.FRW,",",T.tilda.upper.calib.FRW,"]"))
View(pred.int.df)
colnames(pred.int.df) <- c("Naive 90% Prediction Interval", "Calibrated 90% Prediction Interval")
colnames(pred.int.df) <- c("Naive 90% \nPrediction Interval", "Calibrated 90% \nPrediction Interval")
knitr::kable(pred.int.df, align = rep('c', 2), table.envir = 'table*', escape = FALSE, digits = 3) %>%
kableExtra::kable_styling("striped", full_width = T, position = "left")
colnames(pred.int.df) <- c("Naive 90% Prediction Interval", "Calibrated 90% Prediction Interval")
knitr::kable(pred.int.df, align = rep('c', 2), table.envir = 'table*', escape = FALSE, digits = 3) %>%
kableExtra::kable_styling("striped", full_width = T, position = "left")
pred.int.df <- data.frame(naive = paste0("[",T.tilda.lower,", ",T.tilda.upper,"]"),
calibrated = paste0("[",T.tilda.lower.calib.FRW,", ",T.tilda.upper.calib.FRW,"]"))
colnames(pred.int.df) <- c("Naive 90% Prediction Interval", "Calibrated 90% Prediction Interval")
knitr::kable(pred.int.df, align = rep('c', 2), table.envir = 'table*', escape = FALSE, digits = 3) %>%
kableExtra::kable_styling("striped", full_width = T, position = "left")
View(MechanicalSwitch)
R.version
